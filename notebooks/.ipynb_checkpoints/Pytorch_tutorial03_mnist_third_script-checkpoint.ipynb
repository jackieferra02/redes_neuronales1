{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a5d920-3e27-4f6d-82dd-a1966de33aa3",
   "metadata": {},
   "source": [
    "# Learning pytorch by training using MNIST \n",
    "\n",
    "Third script we train feed forward networks and fully connected operator. We start using some good stuff provided by pytorch. \n",
    "\n",
    "Things observed in this code: \n",
    "\n",
    "* -This code starts using the nn.Module class provided by pytorch.\n",
    "\n",
    "* -We will also use the optimizer. However we can always access the parameter list and perform the gradient update as we want. Keras users cannot do this as they do not know how backpropagation works and need an optimizer. Good researchers can invent new update methods and in this case, either they implement their own optimizer or they perform the optimization looping over parameters. Be a researcher and not a keras user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39a719f-7edc-415e-9dd9-714b77363ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to run on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jazfe\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: '[WinError 1920] El sistema no tiene acceso al archivo'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch #main module\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"unable to run on GPU\")\n",
    "else:\n",
    "    print(\"running on CPU\") # Adjusted for CPU\n",
    "import torchvision #computer vision dataset module\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn #Keras users will now be really happy\n",
    "\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f787f7-cb23-496e-a61c-41468930316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Specific parameters of network\n",
    "'''\n",
    "epochs=10\n",
    "batch=100\n",
    "data_len=60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd9c1e3a-a207-45e7-8db4-524f40dc41d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Pytorch Data Loader'''\n",
    "mnist_transforms=transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train=datasets.MNIST('/tmp/', train=True, download=True, transform=mnist_transforms)\n",
    "mnist_test=datasets.MNIST('/tmp/', train=False, download=False, transform=mnist_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd72564-759d-46c8-87b6-990e4cb68844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the dataloader\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)\n",
    "\n",
    "batch=100\n",
    "input_d=784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b288ff3-614f-499a-b518-c409f521b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### USE THE NN MODULE ###################\n",
    "# The nn.Module is a class provided by  pytorch that have fantastic properties. Just refer to the documentation, here I only expose some of them.\n",
    "# We have to override two methods, the __init__ and the forward. The __init__ is used to define the parameters and the forward to perform the\n",
    "# forward operation of the network. \n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.SoftMax = nn.Softmax(dim=1)\n",
    "        self.CE = nn.CrossEntropyLoss() # this performs softmax plus cross-entropy loss\n",
    "        self.F1 = nn.Linear(784, 512) # This will create a Linear operator, i.e it creates a matrix weight and a vector bias. \n",
    "        # These new torch.Tensors with required_grad=True are automatically incorporated as part of the parameter list. The parameter self.F1 is overrided if you now do: self.F1=nn.Linear(512,512). \n",
    "        # Creating dynamic networks (for instance a class Network that implement a fully connected with a variable depth or different topology) can be done using utilities from the nn.Module such as register list or register module, check them. \n",
    "        # One key point, nn.Linear is a nn.Module also, with its __init__ and forward overrided. This means you can create your own operator and then incorporate it in other modules (this is what people do to create Residual Networks, for instance).\n",
    "\n",
    "        # On the other hand, we can create our own operations by defining the parameters. For instance we want to create a second layer of 512 hidden units. We can do it this way:\n",
    "        # First create the parameters needed as nn.Parameters\n",
    "        self.w = nn.Parameter(torch.from_numpy(numpy.random.randn(512, 512) / numpy.sqrt(512)).float())\n",
    "        self.b = nn.Parameter(torch.zeros(512,).float())\n",
    "        # Parameter is  a torch.Tensor that requires_grad. The ONLY difference is that the nn.Module automatically add it to the parameter list. Again the name of the variables\n",
    "        # must be different if you want to register several of them.\n",
    "        # The __init__ method from nn.Linear defines these variables, but with uniform sampling initialization.\n",
    "\n",
    "        # All we have seen can be mixed. Either we use a built-in layer like nn.Linear (which creates a weight and a bias). Or we can create ours, like with self.w and self.b. Now we use a built-in, because we want. The problem is that we might want to change the default initialization. Well, just access the parameters.\n",
    "        self.FO = nn.Linear(512, 10)\n",
    "        # If you want to access the parameters of the Linear Module you can easily do it in this way.\n",
    "        # Change initialization from F1 and FO. It uses uniform distribution by default, however we want to use Gaussian.\n",
    "        self.F1.weight.data = torch.from_numpy(numpy.random.randn(512, 784) / numpy.sqrt(512)).float() \n",
    "        self.FO.weight.data = torch.from_numpy(numpy.random.randn(10, 512) / numpy.sqrt(10)).float()\n",
    "        # Flip dimension. This has to do with blas and cublas library that pytorch uses as backend.\n",
    "\n",
    "    def forward(self, x):   \n",
    "        # we can override x no problem. Pytorch can see they are different variables\n",
    "        x = self.ReLU(self.F1(x))\n",
    "        x = self.ReLU(torch.mm(x, self.w) + self.b) # this is what nn.Linear do inside. The forward() method from nn.Linear does these operations\n",
    "        x = self.FO(x)\n",
    "        return x\n",
    "\n",
    "    def inference(self, x):# I usually create this method to return softmax directly or to control other stuff I will show in other tutorial\n",
    "        x = self.ReLU(self.F1(x))\n",
    "        x = self.ReLU(torch.mm(x, self.w) + self.b)\n",
    "        x = self.SoftMax(self.FO(x))\n",
    "        return x\n",
    "\n",
    "    def Loss(self, t, t_pred):# we can create the methods we want\n",
    "        return self.CE(t_pred, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44182c56-5307-47c4-95f3-696b52495ffc",
   "metadata": {},
   "source": [
    "Este código define una clase Network que hereda de nn.Module, una clase fundamental en PyTorch para construir redes neuronales. La clase Network sobrescribe dos métodos esenciales: __init__ y forward. En __init__, se inicializan las capas de la red, las funciones de activación y la función de pérdida. Se definen dos capas lineales (nn.Linear), una con 784 entradas y 512 salidas (self.F1), y otra con 512 entradas y 10 salidas (self.FO). Además, se crean parámetros personalizados (self.w y self.b) para una segunda capa de 512 unidades ocultas, inicializados manualmente usando distribuciones Gaussianas.\n",
    "\n",
    "El método forward realiza la propagación hacia adelante de la red, aplicando la función de activación ReLU entre las capas. Por otro lado, inference es un método adicional para realizar inferencias, aplicando la función softmax para obtener probabilidades, lo cual es útil para evaluar el rendimiento en el conjunto de prueba. Finalmente, el método Loss calcula la pérdida de entropía cruzada entre las predicciones y las etiquetas reales. En conjunto, este código muestra cómo estructurar una red neuronal usando PyTorch de manera modular, lo que facilita la extensión y el mantenimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90787b27-a5ea-48d6-85f9-5722c5d75fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy 0.246 and Test error 3.150\n",
      "Cross entropy 0.093 and Test error 3.180\n",
      "Cross entropy 0.064 and Test error 2.380\n",
      "Cross entropy 0.048 and Test error 2.930\n",
      "Cross entropy 0.033 and Test error 2.720\n",
      "Cross entropy 0.028 and Test error 2.460\n",
      "Cross entropy 0.025 and Test error 2.140\n",
      "Cross entropy 0.021 and Test error 2.260\n",
      "Cross entropy 0.012 and Test error 1.870\n",
      "Cross entropy 0.007 and Test error 1.720\n"
     ]
    }
   ],
   "source": [
    "# create instance\n",
    "myNet = Network()\n",
    "# myNet.cuda() # move all the registered nn.Parameters and torch.tensor to the gpus,i.e, it moves to gpu everything that involves computation.\n",
    "for e in range(epochs):\n",
    "    MC, ce = [0.0]*2\n",
    "    # now create an optimizer. Calling myNet.parameters() returns a list with all the registered parameters. This is the list of parameters that I referred to when I was creating the nn.Module. Each nn.Parameter is added to this list, and you can access it directly in order to optimize wrt the parameters.\n",
    "    optimizer = torch.optim.SGD(myNet.parameters(), lr=0.1, momentum=0.9)\n",
    "    for x, t in train_loader: # sample one batch\n",
    "        # x, t = x.cuda(), t.cuda()\n",
    "        x = x.view(-1, 784)\n",
    "        o = myNet.forward(x) # forward. o has to be the pre-softmax because the cross entropy loss applies it.\n",
    "        o = myNet.Loss(t, o) # compute loss\n",
    "        o.backward() # this computes the gradient which respect to leaves. And this is the reason for required gradients True\n",
    "        optimizer.step()# step in gradient direction\n",
    "        optimizer.zero_grad()\n",
    "        ce += o.item()\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            # x, t = x.cuda(), t.cuda()\n",
    "            x = x.view(-1, 784)\n",
    "            test_pred = myNet.inference(x)\n",
    "            index = torch.argmax(test_pred, 1)# compute maximum\n",
    "            MC += ((index != t).sum().float()) # accumulate MC error\n",
    "\n",
    "    print(\"Cross entropy {:.3f} and Test error {:.3f}\".format(ce/600., 100*MC/10000.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c31815-b2ec-46f3-a653-ac001a038648",
   "metadata": {},
   "source": [
    "Durante el entrenamiento del modelo, la pérdida de entropía cruzada disminuyó consistentemente con cada época, comenzando en 0.246 y reduciéndose a 0.007 en la última observación. La entropía cruzada es una medida de la diferencia entre las predicciones del modelo y las etiquetas reales; por lo tanto, una disminución constante sugiere que el modelo está mejorando su precisión y ajustándose bien a los datos de entrenamiento.\n",
    "\n",
    "Sin embargo, el error en el conjunto de prueba muestra algunas fluctuaciones. Inicialmente, el error era 3.150%, pero varió durante las épocas, alcanzando un mínimo de 1.720%. Estas fluctuaciones pueden indicar variabilidad en la capacidad del modelo para generalizar a datos no vistos, lo cual es normal en el proceso de entrenamiento de redes neuronales. Lo importante es que, en general, el error de prueba tiende a disminuir, lo que sugiere que el modelo está aprendiendo a manejar datos nuevos de manera más efectiva.\n",
    "\n",
    "En conjunto, estos resultados son prometedores. La disminución en la pérdida de entropía cruzada indica un aprendizaje efectivo y una mejora en la precisión del modelo, mientras que la tendencia decreciente en el error de prueba sugiere una buena capacidad de generalización. Es importante continuar monitoreando estas métricas durante el resto del entrenamiento para asegurar que el modelo no caiga en problemas de sobreajuste y mantenga su capacidad de generalización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc469c6-8d78-497f-b05b-04efcc3ec8e9",
   "metadata": {},
   "source": [
    "* ### **Explicación del código:**\n",
    "\n",
    "* Se importan las librerías necesarias: torch para operaciones tensoriales, torchvision para manipulación de datos de visión por computadora, y numpy para operaciones matemáticas adicionales.\n",
    "* Se definen los parámetros básicos del modelo, como el número de épocas, el tamaño del lote y la longitud de los datos.\n",
    "* Pytorch Data Loader y this is the dataloader: Aquí se cargan los datos MNIST y se aplican las transformaciones necesarias (convertir imágenes a tensores). Los datos se dividen en lotes utilizando DataLoader.\n",
    "* Funciones def: Se define la arquitectura del modelo utilizando nn.Module. Se definen las capas de la red y las funciones de activación, pérdida y un método para la inferencia.\n",
    "* El modelo se entrena durante las épocas definidas. En cada época, se realiza la propagación hacia adelante, se calcula la pérdida, se realiza la retropropagación para actualizar los parámetros y se evalúa el rendimiento en el conjunto de prueba.\n",
    "\n",
    "**Principales Diferencias con el Código Anterior\n",
    "Uso de nn.Module:**\n",
    "\n",
    "* Anterior: No utiliza nn.Module para definir la red.\n",
    "\n",
    "* Actual: Utiliza nn.Module para definir una clase de red estructurada.\n",
    "\n",
    "**Inicialización de Parámetros:**\n",
    "\n",
    "* Anterior: Inicializa parámetros de pesos y sesgos manualmente usando torch.from_numpy.\n",
    "\n",
    "* Actual: Utiliza nn.Linear para inicializar capas y parámetros, facilitando el manejo y organización del modelo.\n",
    "\n",
    "**Método de Inferencia:**\n",
    "\n",
    "* Anterior: No define explícitamente un método de inferencia.\n",
    "\n",
    "* Actual: Define el método inference para realizar predicciones con softmax, útil para la evaluación en el conjunto de prueba.\n",
    "\n",
    "**Modularidad y Organización:**\n",
    "\n",
    "* Anterior: Todo el código está en el script principal.\n",
    "\n",
    "* Actual: El código está más modularizado, con la red definida dentro de una clase, mejorando la claridad y mantenimiento.\n",
    "\n",
    "Este código utiliza buenas prácticas de PyTorch, como el uso de nn.Module y optimizadores integrados, lo que facilita la extensión y modificación del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188abc6-392c-4d40-83cd-b842a1516e8d",
   "metadata": {},
   "source": [
    "Para una competencia piloto, te recomendaría usar el código que emplea nn.Module. Esto es porque es más modular, escalable y eficiente, además de facilitar la integración de nuevas técnicas y optimizaciones.\n",
    "\n",
    "**Pasos para una competencia exitosa:**\n",
    "\n",
    "**Preparación del Modelo:**\n",
    "\n",
    "Utiliza nn.Module para definir una red bien estructurada.\n",
    "\n",
    "Asegúrate de tener optimizadores eficientes y funciones de pérdida bien definidas.\n",
    "\n",
    "**Carga de Datos y Preprocesamiento:**\n",
    "\n",
    "Prepara los datos adecuadamente, aplicando las transformaciones necesarias para mejorar la calidad y el rendimiento del modelo.\n",
    "\n",
    "Usa DataLoader para gestionar eficientemente los lotes de datos.\n",
    "\n",
    "**Entrenamiento del Modelo:**\n",
    "\n",
    "Ejecuta múltiples épocas de entrenamiento, ajustando los hiperparámetros como la tasa de aprendizaje y el tamaño del lote.\n",
    "\n",
    "Monitorea continuamente la pérdida de entrenamiento y el error en el conjunto de prueba para evitar el sobreajuste.\n",
    "\n",
    "**Evaluación y Validación:**\n",
    "\n",
    "Utiliza técnicas de validación cruzada para evaluar el rendimiento del modelo en distintos subconjuntos de datos.\n",
    "\n",
    "Realiza ajustes finos basados en los resultados obtenidos durante la validación.\n",
    "\n",
    "**Optimización y Mejora:**\n",
    "\n",
    "Implementa técnicas de regularización y optimización, como el decaimiento del peso y el aumento de los datos.\n",
    "\n",
    "Experimenta con diferentes arquitecturas de redes para encontrar la más eficaz para tu tarea específica.\n",
    "\n",
    "Siguiendo estos pasos con una red bien estructurada usando nn.Module, tendrás una sólida base para competir y mejorar las probabilidades de ganar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d74d7a-af87-44c8-b094-ac8eb27b593b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (deep_learning)",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
