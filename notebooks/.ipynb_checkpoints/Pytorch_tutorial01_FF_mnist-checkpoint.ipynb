{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "467fe50a-cc2c-4370-bc26-3037c3867c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buy a gpu\n"
     ]
    }
   ],
   "source": [
    "import torch #main module\n",
    "if not torch.cuda.is_available():\n",
    "\tprint(\"Buy a gpu\")\n",
    "\texit(-1)\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import numpy #numpy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec11197-c24e-4f7d-8621-4b545a566377",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specific parameters of network\n",
    "\n",
    "epochs=10\n",
    "batch=100\n",
    "data_len=60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089b72ba-0609-47f4-bb48-e8ebe4c4993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio actual: C:\\Users\\jazfe\\OneDrive\\Documentos\\CUNEF\\redes_neuronales\\Redes_neuronales_1\\notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ver el directorio actual\n",
    "\n",
    "directorio_actual = os.getcwd()\n",
    "print(\"Directorio actual:\", directorio_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51eb8188-f11d-447d-b495-fce73f84be8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo descargado y movido a: data/mnist.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Define la URL y la ruta de destino\n",
    "url = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "destination = 'data/mnist.pkl.gz'\n",
    "\n",
    "# Crear el directorio si no existe\n",
    "os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "\n",
    "# Descargar el archivo\n",
    "urllib.request.urlretrieve(url, destination)\n",
    "\n",
    "print(f'Archivo descargado y movido a: {destination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb2625f-c704-4eb0-9dce-b041255e1eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo se encuentra en la ubicación correcta.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Verifica si el archivo existe en la ruta especificada\n",
    "if os.path.exists('data/mnist.pkl.gz'):\n",
    "    print(\"El archivo se encuentra en la ubicación correcta.\")\n",
    "else:\n",
    "    print(\"El archivo no se encuentra en la ruta especificada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "366421f6-3e28-45e3-b71f-18eab8af62ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD MNIST DATASET\n",
    "\n",
    "#download dataset from Montreal webpage. http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz. Place it in /tmp/\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "# Ruta al archivo dentro del directorio actual\n",
    "dataset = 'data/mnist.pkl.gz'\n",
    "\n",
    "try:\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        try:\n",
    "            in_set_a, valid_set_a, test_set = pickle.load(f, encoding='latin1')  # Añade 'encoding' si es necesario para Python 3\n",
    "        except:\n",
    "            raise NameError(\"Error loading the dataset\")\n",
    "except FileNotFoundError:\n",
    "    raise NameError(\"El archivo no se encuentra en la ruta especificada. Verifica que el archivo 'mnist.pkl.gz' esté en la carpeta 'data'.\")\n",
    "except gzip.BadGzipFile:\n",
    "    raise NameError(\"El archivo no está en formato GZIP válido. Intenta volver a descargarlo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26e36d2-3535-430b-8a2a-c705d3339384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 50000 samples\n",
      "Validation set: 10000 samples\n",
      "Test set: 10000 samples\n",
      "First training sample: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01171875 0.0703125  0.0703125  0.0703125\n",
      " 0.4921875  0.53125    0.68359375 0.1015625  0.6484375  0.99609375\n",
      " 0.96484375 0.49609375 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1171875  0.140625   0.3671875  0.6015625\n",
      " 0.6640625  0.98828125 0.98828125 0.98828125 0.98828125 0.98828125\n",
      " 0.87890625 0.671875   0.98828125 0.9453125  0.76171875 0.25\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19140625\n",
      " 0.9296875  0.98828125 0.98828125 0.98828125 0.98828125 0.98828125\n",
      " 0.98828125 0.98828125 0.98828125 0.98046875 0.36328125 0.3203125\n",
      " 0.3203125  0.21875    0.15234375 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.0703125  0.85546875 0.98828125\n",
      " 0.98828125 0.98828125 0.98828125 0.98828125 0.7734375  0.7109375\n",
      " 0.96484375 0.94140625 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3125     0.609375   0.41796875 0.98828125\n",
      " 0.98828125 0.80078125 0.04296875 0.         0.16796875 0.6015625\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0546875  0.00390625 0.6015625  0.98828125 0.3515625\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54296875 0.98828125 0.7421875  0.0078125  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04296875\n",
      " 0.7421875  0.98828125 0.2734375  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13671875 0.94140625\n",
      " 0.87890625 0.625      0.421875   0.00390625 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31640625 0.9375     0.98828125\n",
      " 0.98828125 0.46484375 0.09765625 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17578125 0.7265625  0.98828125 0.98828125\n",
      " 0.5859375  0.10546875 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0625     0.36328125 0.984375   0.98828125 0.73046875\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97265625 0.98828125 0.97265625 0.25       0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1796875  0.5078125  0.71484375 0.98828125\n",
      " 0.98828125 0.80859375 0.0078125  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15234375 0.578125\n",
      " 0.89453125 0.98828125 0.98828125 0.98828125 0.9765625  0.7109375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09375    0.4453125  0.86328125 0.98828125 0.98828125 0.98828125\n",
      " 0.98828125 0.78515625 0.3046875  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.08984375 0.2578125  0.83203125 0.98828125\n",
      " 0.98828125 0.98828125 0.98828125 0.7734375  0.31640625 0.0078125\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.0703125  0.66796875\n",
      " 0.85546875 0.98828125 0.98828125 0.98828125 0.98828125 0.76171875\n",
      " 0.3125     0.03515625 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21484375 0.671875   0.8828125  0.98828125 0.98828125 0.98828125\n",
      " 0.98828125 0.953125   0.51953125 0.04296875 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53125    0.98828125\n",
      " 0.98828125 0.98828125 0.828125   0.52734375 0.515625   0.0625\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Verifica el contenido de los datos cargados\n",
    "print(f\"Train set: {len(in_set_a[0])} samples\")\n",
    "print(f\"Validation set: {len(valid_set_a[0])} samples\")\n",
    "print(f\"Test set: {len(test_set[0])} samples\")\n",
    "\n",
    "# Ejemplo de acceso a un dato específico\n",
    "print(f\"First training sample: {in_set_a[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178bd33b-d73f-4149-98dd-0561170c1bc2",
   "metadata": {},
   "source": [
    "**Verificación de la Carga de Datos**\n",
    "\n",
    "**Número de Muestras:**\n",
    "\n",
    "* Train set: 50000 samples: El conjunto de entrenamiento contiene 50,000 muestras.\n",
    "\n",
    "* Validation set: 10000 samples: El conjunto de validación contiene 10,000 muestras.\n",
    "\n",
    "* Test set: 10000 samples: El conjunto de prueba contiene 10,000 muestras.\n",
    "\n",
    "**Primer Ejemplo de Entrenamiento:**\n",
    "\n",
    "La salida muestra los valores de los píxeles del primer ejemplo en el conjunto de entrenamiento. Estos valores son números entre 0 y 1, que representan la intensidad de los píxeles en una imagen normalizada del dígito MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f440f006-77a0-4b59-b774-83f3de781b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Asigna los datos cargados a las variables adecuadas\n",
    "train_feat = in_set_a[0]\n",
    "train_labl = in_set_a[1]\n",
    "\n",
    "# Prepara tus datos\n",
    "train_features = torch.tensor(train_feat, dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_labl, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb3d5cca-6273-4381-b4ad-0fa7185c7466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear TensorDataset\n",
    "mnist_dataset_train = TensorDataset(train_features, train_labels)\n",
    "\n",
    "# Crear DataLoader\n",
    "train_loader = DataLoader(mnist_dataset_train, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "165051bd-5dce-4c7e-8380-2d45cfa59fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define functions\n",
    "'''\n",
    "#Define a function with the typical activations. This activations can be obtained from the nn.Module. \n",
    "def activation(x,tip):\n",
    "\tif tip==\"sof\":\n",
    "\t\tf=nn.Softmax(dim=1)\n",
    "\t\treturn f(x)\n",
    "\telif tip=='relu':\n",
    "\t\tf=nn.ReLU()\n",
    "\t\treturn f(x)\n",
    "\telse:\n",
    "\t\tprint(\"activation function not present\")\n",
    "\t\texit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47285c43-08a1-4fbd-9d2b-39eae146a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward operation. \n",
    "def forward(x):\n",
    "\tx2 = activation(torch.mm(x,w1)+b1,'relu')\n",
    "\ty_pre_activation = torch.mm(x2,w2)+b2\n",
    "\treturn y_pre_activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c283e2e-24cc-4dc6-9815-b223e635fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference. We can use the forward function, however we explicitly apply softmax here though it is not necessary to compute the argmax.\n",
    "#In the next tutorials I cover when, in my opinion, I like make this difference explictly. A reason could be if you want to directly return the softmax output.\n",
    "#This is because, in pytorch, the crossentropy loss applies this activation\n",
    "def inference(x):\n",
    "\tx2 = activation(torch.mm(x,w1)+b1,'relu')\n",
    "\tpreactivation =activation( torch.mm(x2,w2)+b2,'sof')\n",
    "\treturn preactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dabdde33-5b63-42ba-b69e-2302df6626b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "CE = nn.CrossEntropyLoss() #this performs softmax plus cros-entropy\n",
    "def loss(t_,t):\n",
    "\treturn CE(t_,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59e112f0-3610-482f-8c50-fe2f43b60c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define your layer dimension. We train a fully connected neural network of only one layer with 512 neurons\n",
    "l1=512\n",
    "l2=10\n",
    "input_d=784\n",
    "target_d=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "340ade2d-0a11-4407-bcc2-e08df72e3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Crear parámetros de la red (variables compartidas en Theano)\n",
    "w1 = torch.from_numpy(np.random.normal(0, np.sqrt(2.0 / float(l1)), (input_d, l1)).astype('float32')).requires_grad_()\n",
    "w2 = torch.from_numpy(np.random.normal(0, np.sqrt(2.0 / float(l2)), (l1, l2)).astype('float32')).requires_grad_()\n",
    "b1 = torch.zeros((1, l1)).requires_grad_()\n",
    "b2 = torch.zeros((1, l2)).requires_grad_()\n",
    "differentiated_params = [w1, w2, b1, b2]\n",
    "\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5de68f7e-2805-4067-ae59-a6bd60cfa789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 cross entropy 0.53617 and Test error 8.720\n",
      "Epoch 1 cross entropy 0.25945 and Test error 6.770\n",
      "Epoch 2 cross entropy 0.20718 and Test error 5.930\n",
      "Epoch 3 cross entropy 0.17654 and Test error 5.430\n",
      "Epoch 4 cross entropy 0.15505 and Test error 4.930\n",
      "Epoch 5 cross entropy 0.13880 and Test error 4.650\n",
      "Epoch 6 cross entropy 0.12601 and Test error 4.470\n",
      "Epoch 7 cross entropy 0.11529 and Test error 4.290\n",
      "Epoch 8 cross entropy 0.10661 and Test error 4.080\n",
      "Epoch 9 cross entropy 0.09950 and Test error 3.880\n",
      "Epoch 10 cross entropy 0.09284 and Test error 3.840\n",
      "Epoch 11 cross entropy 0.08718 and Test error 3.820\n",
      "Epoch 12 cross entropy 0.08226 and Test error 3.590\n",
      "Epoch 13 cross entropy 0.07772 and Test error 3.560\n",
      "Epoch 14 cross entropy 0.07358 and Test error 3.560\n",
      "Epoch 15 cross entropy 0.07002 and Test error 3.350\n",
      "Epoch 16 cross entropy 0.06684 and Test error 3.360\n",
      "Epoch 17 cross entropy 0.06386 and Test error 3.270\n",
      "Epoch 18 cross entropy 0.06116 and Test error 3.250\n",
      "Epoch 19 cross entropy 0.05856 and Test error 3.250\n"
     ]
    }
   ],
   "source": [
    "# Y ahora el bucle principal: forward, cost, backward, update, reset grads\n",
    "for e in range(epochs):\n",
    "    ce = 0.0\n",
    "    for x, t in train_loader:  # Sample one batch\n",
    "        # No mover a GPU, trabajar en CPU\n",
    "        predict = forward(x)  # Forward\n",
    "        o = loss(predict, t)  # Compute loss\n",
    "        o.backward()  # Compute the gradient which respect to leaves\n",
    "\n",
    "        ce += o.data.item()  # Store the ce for printing\n",
    "\n",
    "        for p in differentiated_params:  # Loop over params\n",
    "            p.data = p.data - 0.01 * p.grad.data  # Gradient descent\n",
    "            p.grad.data.zero_()  # Reset the gradient\n",
    "\n",
    "    # Imprimir la pérdida y el error en el conjunto de prueba\n",
    "    test_features = torch.tensor(test_set[0], dtype=torch.float32)\n",
    "    test_labels = torch.tensor(test_set[1], dtype=torch.int64)\n",
    "    test_pred = inference(test_features)  # Compute the softmax from test data\n",
    "    index = torch.argmax(test_pred, 1).data  # Compute argument maximum\n",
    "    MC = (index != test_labels).float().mean().item() * 100  # Compute MC error in % | Porcentaje de error\n",
    "    print(f\"Epoch {e} cross entropy {ce / len(train_loader):.5f} and Test error {MC:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79acb18f-dc6e-4503-90c8-3b6abdb65a62",
   "metadata": {},
   "source": [
    "**Medidas de Rendimiento**\n",
    "\n",
    "Las medidas de rendimiento son métricas que se utilizan para evaluar la efectividad de un modelo de aprendizaje automático. Algunas de las medidas más comunes incluyen:\n",
    "\n",
    "* Loss (Pérdida):\n",
    "\n",
    "La pérdida es una medida de cuán lejos están las predicciones del modelo de los valores reales. Una pérdida más baja indica que el modelo está haciendo mejores predicciones.\n",
    "\n",
    "En tu caso, estás utilizando la pérdida de cross entropy (entropía cruzada) para medir cuán bien está funcionando tu modelo en la tarea de clasificación.\n",
    "\n",
    "* Test Error (Error de Prueba):\n",
    "\n",
    "El error de prueba es el porcentaje de predicciones incorrectas que realiza el modelo en el conjunto de datos de prueba. Un error más bajo indica que el modelo tiene un mejor rendimiento en datos no vistos durante el entrenamiento.\n",
    "\n",
    "**Interpretación de los resultados**\n",
    "\n",
    "* Disminución de la Cross Entropy:\n",
    "\n",
    "Observa cómo la cross entropy (pérdida) disminuye progresivamente con cada epoch. Esto indica que el modelo está aprendiendo y mejorando su capacidad para hacer predicciones correctas durante el entrenamiento.\n",
    "\n",
    "Por ejemplo, la pérdida disminuye de 0.53617 en la epoch 0 a 0.05856 en la epoch 19.\n",
    "\n",
    "* Reducción del Error de Prueba:\n",
    "\n",
    "El error de prueba también disminuye con cada epoch, lo que sugiere que el modelo está generalizando bien y es capaz de hacer predicciones precisas en datos no vistos.\n",
    "\n",
    "El error de prueba baja de 8.720% en la epoch 0 a 3.250% en la epoch 19.\n",
    "\n",
    "**Conclusión**\n",
    "\n",
    "Mejoras Progresivas: Tanto la pérdida de entropía cruzada como el error de prueba muestran una tendencia descendente, indicando que tu modelo está aprendiendo correctamente y mejorando su rendimiento con cada epoch.\n",
    "\n",
    "Buen Rendimiento: Al final del entrenamiento, tienes una cross entropy baja y un error de prueba reducido, lo que sugiere que tu modelo está bien entrenado y realiza predicciones precisas.\n",
    "\n",
    "Estas métricas son fundamentales para evaluar y comparar el rendimiento de tu modelo a lo largo del tiempo y asegurarte de que está aprendiendo y generalizando correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "610801a1-93dd-4ccf-b120-ddfd4a7244ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip \n",
    "import pickle \n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90f668ce-17a1-46f8-abea-b2a296c39b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 0 cross entropy 0.60262 and Validation error 9.610\n",
      "Fold 5 Epoch 1 cross entropy 0.29054 and Validation error 7.860\n",
      "Fold 5 Epoch 2 cross entropy 0.23082 and Validation error 6.960\n",
      "Fold 5 Epoch 3 cross entropy 0.19653 and Validation error 6.420\n",
      "Fold 5 Epoch 4 cross entropy 0.17324 and Validation error 5.900\n",
      "Fold 5 Epoch 5 cross entropy 0.15538 and Validation error 5.290\n",
      "Fold 5 Epoch 6 cross entropy 0.14139 and Validation error 5.150\n",
      "Fold 5 Epoch 7 cross entropy 0.12939 and Validation error 4.980\n",
      "Fold 5 Epoch 8 cross entropy 0.11993 and Validation error 4.800\n",
      "Fold 5 Epoch 9 cross entropy 0.11149 and Validation error 4.670\n",
      "Fold 5 Epoch 10 cross entropy 0.10427 and Validation error 4.450\n",
      "Fold 5 Epoch 11 cross entropy 0.09784 and Validation error 4.310\n",
      "Fold 5 Epoch 12 cross entropy 0.09229 and Validation error 4.140\n",
      "Fold 5 Epoch 13 cross entropy 0.08700 and Validation error 4.130\n",
      "Fold 5 Epoch 14 cross entropy 0.08291 and Validation error 4.000\n",
      "Fold 5 Epoch 15 cross entropy 0.07876 and Validation error 4.010\n",
      "Fold 5 Epoch 16 cross entropy 0.07488 and Validation error 3.870\n",
      "Fold 5 Epoch 17 cross entropy 0.07125 and Validation error 3.930\n",
      "Fold 5 Epoch 18 cross entropy 0.06841 and Validation error 3.850\n",
      "Fold 5 Epoch 19 cross entropy 0.06551 and Validation error 3.760\n",
      "Fold 5 Epoch 0 cross entropy 0.59658 and Validation error 10.230\n",
      "Fold 5 Epoch 1 cross entropy 0.28860 and Validation error 8.150\n",
      "Fold 5 Epoch 2 cross entropy 0.22802 and Validation error 7.180\n",
      "Fold 5 Epoch 3 cross entropy 0.19437 and Validation error 6.570\n",
      "Fold 5 Epoch 4 cross entropy 0.17065 and Validation error 6.010\n",
      "Fold 5 Epoch 5 cross entropy 0.15290 and Validation error 5.780\n",
      "Fold 5 Epoch 6 cross entropy 0.13937 and Validation error 5.400\n",
      "Fold 5 Epoch 7 cross entropy 0.12817 and Validation error 5.170\n",
      "Fold 5 Epoch 8 cross entropy 0.11850 and Validation error 5.020\n",
      "Fold 5 Epoch 9 cross entropy 0.11042 and Validation error 4.810\n",
      "Fold 5 Epoch 10 cross entropy 0.10347 and Validation error 4.650\n",
      "Fold 5 Epoch 11 cross entropy 0.09727 and Validation error 4.480\n",
      "Fold 5 Epoch 12 cross entropy 0.09171 and Validation error 4.430\n",
      "Fold 5 Epoch 13 cross entropy 0.08727 and Validation error 4.230\n",
      "Fold 5 Epoch 14 cross entropy 0.08255 and Validation error 4.170\n",
      "Fold 5 Epoch 15 cross entropy 0.07847 and Validation error 4.090\n",
      "Fold 5 Epoch 16 cross entropy 0.07505 and Validation error 4.120\n",
      "Fold 5 Epoch 17 cross entropy 0.07185 and Validation error 4.030\n",
      "Fold 5 Epoch 18 cross entropy 0.06866 and Validation error 3.840\n",
      "Fold 5 Epoch 19 cross entropy 0.06559 and Validation error 3.840\n",
      "Fold 5 Epoch 0 cross entropy 0.60915 and Validation error 10.200\n",
      "Fold 5 Epoch 1 cross entropy 0.28608 and Validation error 7.980\n",
      "Fold 5 Epoch 2 cross entropy 0.22550 and Validation error 7.070\n",
      "Fold 5 Epoch 3 cross entropy 0.19150 and Validation error 6.450\n",
      "Fold 5 Epoch 4 cross entropy 0.16770 and Validation error 5.980\n",
      "Fold 5 Epoch 5 cross entropy 0.14953 and Validation error 5.750\n",
      "Fold 5 Epoch 6 cross entropy 0.13568 and Validation error 5.500\n",
      "Fold 5 Epoch 7 cross entropy 0.12434 and Validation error 5.200\n",
      "Fold 5 Epoch 8 cross entropy 0.11493 and Validation error 5.050\n",
      "Fold 5 Epoch 9 cross entropy 0.10684 and Validation error 4.780\n",
      "Fold 5 Epoch 10 cross entropy 0.09983 and Validation error 4.720\n",
      "Fold 5 Epoch 11 cross entropy 0.09400 and Validation error 4.640\n",
      "Fold 5 Epoch 12 cross entropy 0.08861 and Validation error 4.460\n",
      "Fold 5 Epoch 13 cross entropy 0.08384 and Validation error 4.370\n",
      "Fold 5 Epoch 14 cross entropy 0.07939 and Validation error 4.300\n",
      "Fold 5 Epoch 15 cross entropy 0.07541 and Validation error 4.210\n",
      "Fold 5 Epoch 16 cross entropy 0.07202 and Validation error 4.110\n",
      "Fold 5 Epoch 17 cross entropy 0.06888 and Validation error 4.180\n",
      "Fold 5 Epoch 18 cross entropy 0.06577 and Validation error 4.110\n",
      "Fold 5 Epoch 19 cross entropy 0.06297 and Validation error 4.080\n",
      "Fold 5 Epoch 0 cross entropy 0.59059 and Validation error 10.700\n",
      "Fold 5 Epoch 1 cross entropy 0.29153 and Validation error 8.560\n",
      "Fold 5 Epoch 2 cross entropy 0.23113 and Validation error 7.400\n",
      "Fold 5 Epoch 3 cross entropy 0.19645 and Validation error 6.610\n",
      "Fold 5 Epoch 4 cross entropy 0.17239 and Validation error 6.150\n",
      "Fold 5 Epoch 5 cross entropy 0.15413 and Validation error 5.760\n",
      "Fold 5 Epoch 6 cross entropy 0.14008 and Validation error 5.530\n",
      "Fold 5 Epoch 7 cross entropy 0.12862 and Validation error 5.450\n",
      "Fold 5 Epoch 8 cross entropy 0.11850 and Validation error 5.160\n",
      "Fold 5 Epoch 9 cross entropy 0.11009 and Validation error 5.070\n",
      "Fold 5 Epoch 10 cross entropy 0.10318 and Validation error 4.780\n",
      "Fold 5 Epoch 11 cross entropy 0.09709 and Validation error 4.600\n",
      "Fold 5 Epoch 12 cross entropy 0.09134 and Validation error 4.540\n",
      "Fold 5 Epoch 13 cross entropy 0.08641 and Validation error 4.310\n",
      "Fold 5 Epoch 14 cross entropy 0.08170 and Validation error 4.300\n",
      "Fold 5 Epoch 15 cross entropy 0.07768 and Validation error 4.320\n",
      "Fold 5 Epoch 16 cross entropy 0.07379 and Validation error 4.140\n",
      "Fold 5 Epoch 17 cross entropy 0.07034 and Validation error 4.060\n",
      "Fold 5 Epoch 18 cross entropy 0.06712 and Validation error 4.110\n",
      "Fold 5 Epoch 19 cross entropy 0.06428 and Validation error 4.050\n",
      "Fold 5 Epoch 0 cross entropy 0.60703 and Validation error 10.600\n",
      "Fold 5 Epoch 1 cross entropy 0.28251 and Validation error 8.170\n",
      "Fold 5 Epoch 2 cross entropy 0.22165 and Validation error 7.160\n",
      "Fold 5 Epoch 3 cross entropy 0.18686 and Validation error 6.480\n",
      "Fold 5 Epoch 4 cross entropy 0.16291 and Validation error 6.210\n",
      "Fold 5 Epoch 5 cross entropy 0.14577 and Validation error 5.600\n",
      "Fold 5 Epoch 6 cross entropy 0.13181 and Validation error 5.430\n",
      "Fold 5 Epoch 7 cross entropy 0.12063 and Validation error 5.170\n",
      "Fold 5 Epoch 8 cross entropy 0.11114 and Validation error 5.130\n",
      "Fold 5 Epoch 9 cross entropy 0.10303 and Validation error 4.910\n",
      "Fold 5 Epoch 10 cross entropy 0.09632 and Validation error 4.790\n",
      "Fold 5 Epoch 11 cross entropy 0.09024 and Validation error 4.760\n",
      "Fold 5 Epoch 12 cross entropy 0.08487 and Validation error 4.590\n",
      "Fold 5 Epoch 13 cross entropy 0.08003 and Validation error 4.590\n",
      "Fold 5 Epoch 14 cross entropy 0.07601 and Validation error 4.630\n",
      "Fold 5 Epoch 15 cross entropy 0.07229 and Validation error 4.600\n",
      "Fold 5 Epoch 16 cross entropy 0.06884 and Validation error 4.290\n",
      "Fold 5 Epoch 17 cross entropy 0.06560 and Validation error 4.440\n",
      "Fold 5 Epoch 18 cross entropy 0.06283 and Validation error 4.330\n",
      "Fold 5 Epoch 19 cross entropy 0.06003 and Validation error 4.250\n",
      "Final Test error 3.620\n"
     ]
    }
   ],
   "source": [
    "# Definir el número de divisiones para KFold\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for train_index, val_index in kf.split(train_feat):\n",
    "    X_train, X_val = train_feat[train_index], train_feat[val_index]\n",
    "    y_train, y_val = train_labl[train_index], train_labl[val_index]\n",
    "    \n",
    "    # Convertir a tensores de PyTorch\n",
    "    train_features = torch.tensor(X_train, dtype=torch.float32)\n",
    "    train_labels = torch.tensor(y_train, dtype=torch.int64)\n",
    "    val_features = torch.tensor(X_val, dtype=torch.float32)\n",
    "    val_labels = torch.tensor(y_val, dtype=torch.int64)\n",
    "    \n",
    "    # Crear TensorDataset y DataLoader\n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    val_dataset = TensorDataset(val_features, val_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
    "    \n",
    "    # Reinicializar los parámetros de la red para cada fold\n",
    "    w1 = torch.from_numpy(np.random.normal(0, np.sqrt(2.0 / float(l1)), (input_d, l1)).astype('float32')).requires_grad_()\n",
    "    w2 = torch.from_numpy(np.random.normal(0, np.sqrt(2.0 / float(l2)), (l1, l2)).astype('float32')).requires_grad_()\n",
    "    b1 = torch.zeros((1, l1)).requires_grad_()\n",
    "    b2 = torch.zeros((1, l2)).requires_grad_()\n",
    "    differentiated_params = [w1, w2, b1, b2]\n",
    "\n",
    "    # Entrenamiento\n",
    "    for e in range(epochs):\n",
    "        ce = 0.0\n",
    "        for x, t in train_loader:\n",
    "            predict = forward(x)  # Forward\n",
    "            o = loss(predict, t)  # Compute loss\n",
    "            o.backward()  # Compute the gradient which respect to leaves\n",
    "            ce += o.data.item()  # Store the ce for printing\n",
    "\n",
    "            for p in differentiated_params:  # Loop over params\n",
    "                p.data = p.data - 0.01 * p.grad.data  # Gradient descent\n",
    "                p.grad.data.zero_()  # Reset the gradient\n",
    "\n",
    "        # Evaluación en el conjunto de validación\n",
    "        val_pred = inference(val_features)\n",
    "        val_index = torch.argmax(val_pred, 1).data\n",
    "        val_MC = (val_index != val_labels).float().mean().item() * 100  # Compute validation error\n",
    "        print(f\"Fold {kf.get_n_splits()} Epoch {e} cross entropy {ce / len(train_loader):.5f} and Validation error {val_MC:.3f}\")\n",
    "\n",
    "# Evaluar en el conjunto de prueba final\n",
    "test_pred = inference(test_features)\n",
    "test_index = torch.argmax(test_pred, 1).data\n",
    "test_MC = (test_index != test_labels).float().mean().item() * 100  # Compute test error\n",
    "print(f\"Final Test error {test_MC:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a94ab0-cc4a-4e1e-ab37-be9a75dd0993",
   "metadata": {},
   "source": [
    "**Interpretación de los Resultados Finales**\n",
    "\n",
    "Vamos a analizar y entender qué significan estos resultados de entrenamiento y validación:\n",
    "\n",
    "1. Cross Entropy (Pérdida)\n",
    "Cross Entropy mide la diferencia entre las distribuciones de probabilidad predicha y verdadera.\n",
    "\n",
    "Una cross entropy baja significa que tu modelo está prediciendo de manera más precisa.\n",
    "\n",
    "En las últimas epochs, la cross entropy disminuyó progresivamente, alcanzando 0.06003 en la Epoch 19. Esto indica que el modelo está aprendiendo correctamente y ajustándose bien a los datos de entrenamiento.\n",
    "\n",
    "2. Validation Error (Error de Validación)\n",
    "Validation Error es el porcentaje de predicciones incorrectas en el conjunto de datos de validación.\n",
    "\n",
    "Un validation error bajo indica que el modelo tiene un buen rendimiento en datos no vistos durante el entrenamiento.\n",
    "\n",
    "Aunque hay algunas fluctuaciones, en general, el validation error muestra una tendencia a la baja, alcanzando 4.250% en la Epoch 19. Esto sugiere que el modelo está mejorando su capacidad de generalización.\n",
    "\n",
    "3. Final Test Error (Error de Prueba)\n",
    "Test Error es el porcentaje de predicciones incorrectas en el conjunto de datos de prueba.\n",
    "\n",
    "Un test error bajo es deseable, ya que indica que el modelo puede hacer predicciones precisas en datos completamente nuevos y no vistos.\n",
    "\n",
    "Tu modelo logró un Final Test Error de **3.620%**, lo cual es un buen resultado. Esto significa que tu modelo tiene un rendimiento sólido y es capaz de generalizar bien a datos no vistos.\n",
    "\n",
    "**Conclusión**\n",
    "* Buen Rendimiento del Modelo: Tanto la cross entropy como el validation error han mostrado una mejora constante a lo largo del entrenamiento, lo que indica que el modelo está aprendiendo y generalizando bien.\n",
    "\n",
    "* Test Error Bajo: Un Final Test Error de 3.620% sugiere que el modelo está haciendo un buen trabajo al predecir correctamente la mayoría de las muestras en el conjunto de prueba.\n",
    "\n",
    "* Potencial para Mejoras: Aunque los resultados son buenos, siempre hay espacio para ajustar hiperparámetros, probar diferentes arquitecturas, y utilizar técnicas de regularización para posiblemente mejorar aún más el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba209a9c-2483-4cdc-9a12-3df4895fb1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (deep_learning)",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
