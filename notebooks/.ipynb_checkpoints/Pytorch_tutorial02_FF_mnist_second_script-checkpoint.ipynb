{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bcfc22f-aac4-42af-854c-dd078c7cc3e8",
   "metadata": {},
   "source": [
    "## Learning pytorch by training using MNIST\n",
    "\n",
    "Second script we train feed forward networks and fully connected operator. We start moving from theano based code \n",
    "to Keras based code.\n",
    "\n",
    "## Things observed in this code:\n",
    "\n",
    "* How can we optmize the parameters easily.\n",
    "\n",
    "* how can we use the torchvision interface to load a provided dataset.\n",
    "\n",
    "* what is torch.no_grad() and how to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a367c4-ebb9-482f-9bc5-e10941523868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buy a gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jazfe\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: '[WinError 1920] El sistema no tiene acceso al archivo'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch #main module\n",
    "if not torch.cuda.is_available():\n",
    "\tprint(\"Buy a gpu\")\n",
    "\texit(-1)\n",
    "import torchvision #computer vision dataset module\n",
    "from torchvision import datasets,transforms\n",
    "from torch import nn #not use in this tutorials. Perfect for weepers and for Keras Users.\n",
    "\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d289ee-2e8d-4d08-bab7-5ca3b0be601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import numpy \n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# se importan las librerías necesarias, incluyendo PyTorch, numpy y torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a8b1ae1-34a5-4f3b-aa5e-bcfa4a7c71bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /tmp/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/MNIST\\raw\\train-images-idx3-ubyte.gz to /tmp/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /tmp/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/MNIST\\raw\\train-labels-idx1-ubyte.gz to /tmp/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /tmp/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/MNIST\\raw\\t10k-images-idx3-ubyte.gz to /tmp/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /tmp/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to /tmp/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformaciones y carga de datos\n",
    "mnist_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_train = datasets.MNIST('/tmp/', train=True, download=True, transform=mnist_transforms)\n",
    "mnist_test = datasets.MNIST('/tmp/', train=False, download=False, transform=mnist_transforms)\n",
    "\n",
    "# Se definen las transformaciones (convertir imágenes a tensores) y se descargan/cargan los conjuntos de datos MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b61a0674-d96f-455b-9424-1fa07086e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=True)\n",
    "\n",
    "# Se crean los cargadores de datos (DataLoader) para el entrenamiento y prueba, \n",
    "# que permiten manejar los datos en lotes de 100 y barajarlos en cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23f1b0df-541f-41a6-a196-d1e2d36a3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "l1, l2, l3 = 512, 512, 10\n",
    "input_d, target_d, batch, epochs = 784, 10, 100, 50\n",
    "\n",
    "# Se establecen los parámetros para las dimensiones de las capas y el entrenamiento, \n",
    "# como el tamaño del lote y el número de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd0ba46a-e0eb-476c-b032-f30670a9cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de pesos y bias\n",
    "w1 = torch.from_numpy(numpy.random.normal(0, numpy.sqrt(2.0/float(l1)), (input_d, l1)).astype('float32')).requires_grad_()\n",
    "w2 = torch.from_numpy(numpy.random.normal(0, numpy.sqrt(2.0/float(l2)), (l1, l2)).astype('float32')).requires_grad_()\n",
    "w3 = torch.from_numpy(numpy.random.normal(0, numpy.sqrt(2.0/float(l2)), (l2, l3)).astype('float32')).requires_grad_()\n",
    "\n",
    "b1 = torch.from_numpy(numpy.zeros((1, l1))).float().requires_grad_()\n",
    "b2 = torch.from_numpy(numpy.zeros((1, l2))).float().requires_grad_()\n",
    "b3 = torch.from_numpy(numpy.zeros((1, l3))).float().requires_grad_()\n",
    "\n",
    "fRl = nn.ReLU()\n",
    "\n",
    "# Se inicializan los pesos y sesgos del modelo con valores aleatorios y se establece que requieren gradientes para el entrenamiento.\n",
    "\n",
    "# Se define la función de activación ReLU y la función de pérdida de entropía cruzada (abajo, CrossEntropyLoss), \n",
    "# que se utilizarán durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b3b722d-6016-40cf-9a47-815be44b934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimización\n",
    "optimized_params = [b1, b2, b3, w1, w2, w3]\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(optimized_params, lr=0.1, momentum=0.9)\n",
    "\n",
    "# Se configuran los parámetros a optimizar y se utiliza el optimizador SGD con una tasa de aprendizaje y momento especificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3d4bac9-96a4-4d17-b8b9-c1cc8de9ff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| Epoch 0 cross entropy 0.20941 and Test error 3.070\n",
      "|| Epoch 1 cross entropy 0.08379 and Test error 3.200\n",
      "|| Epoch 2 cross entropy 0.05468 and Test error 2.640\n",
      "|| Epoch 3 cross entropy 0.03836 and Test error 2.190\n",
      "|| Epoch 4 cross entropy 0.02688 and Test error 2.010\n",
      "|| Epoch 5 cross entropy 0.01847 and Test error 1.860\n",
      "|| Epoch 6 cross entropy 0.01177 and Test error 1.710\n",
      "|| Epoch 7 cross entropy 0.01321 and Test error 1.970\n",
      "|| Epoch 8 cross entropy 0.01197 and Test error 1.770\n",
      "|| Epoch 9 cross entropy 0.00426 and Test error 1.880\n",
      "|| Epoch 10 cross entropy 0.00411 and Test error 2.020\n",
      "|| Epoch 11 cross entropy 0.00538 and Test error 1.780\n",
      "|| Epoch 12 cross entropy 0.00437 and Test error 1.770\n",
      "|| Epoch 13 cross entropy 0.00453 and Test error 1.630\n",
      "|| Epoch 14 cross entropy 0.00226 and Test error 1.610\n",
      "|| Epoch 15 cross entropy 0.00082 and Test error 1.460\n",
      "|| Epoch 16 cross entropy 0.00014 and Test error 1.410\n",
      "|| Epoch 17 cross entropy 0.00007 and Test error 1.400\n",
      "|| Epoch 18 cross entropy 0.00006 and Test error 1.420\n",
      "|| Epoch 19 cross entropy 0.00005 and Test error 1.400\n",
      "|| Epoch 20 cross entropy 0.00005 and Test error 1.420\n",
      "|| Epoch 21 cross entropy 0.00004 and Test error 1.400\n",
      "|| Epoch 22 cross entropy 0.00004 and Test error 1.410\n",
      "|| Epoch 23 cross entropy 0.00004 and Test error 1.410\n",
      "|| Epoch 24 cross entropy 0.00003 and Test error 1.420\n",
      "|| Epoch 25 cross entropy 0.00003 and Test error 1.420\n",
      "|| Epoch 26 cross entropy 0.00003 and Test error 1.420\n",
      "|| Epoch 27 cross entropy 0.00003 and Test error 1.420\n",
      "|| Epoch 28 cross entropy 0.00003 and Test error 1.400\n",
      "|| Epoch 29 cross entropy 0.00002 and Test error 1.400\n",
      "|| Epoch 30 cross entropy 0.00002 and Test error 1.410\n",
      "|| Epoch 31 cross entropy 0.00002 and Test error 1.400\n",
      "|| Epoch 32 cross entropy 0.00002 and Test error 1.390\n",
      "|| Epoch 33 cross entropy 0.00002 and Test error 1.390\n",
      "|| Epoch 34 cross entropy 0.00002 and Test error 1.400\n",
      "|| Epoch 35 cross entropy 0.00002 and Test error 1.390\n",
      "|| Epoch 36 cross entropy 0.00002 and Test error 1.390\n",
      "|| Epoch 37 cross entropy 0.00002 and Test error 1.390\n",
      "|| Epoch 38 cross entropy 0.00002 and Test error 1.390\n",
      "|| Epoch 39 cross entropy 0.00002 and Test error 1.400\n",
      "|| Epoch 40 cross entropy 0.00002 and Test error 1.400\n",
      "|| Epoch 41 cross entropy 0.00002 and Test error 1.400\n",
      "|| Epoch 42 cross entropy 0.00002 and Test error 1.400\n",
      "|| Epoch 43 cross entropy 0.00002 and Test error 1.400\n",
      "|| Epoch 44 cross entropy 0.00001 and Test error 1.400\n",
      "|| Epoch 45 cross entropy 0.00001 and Test error 1.410\n",
      "|| Epoch 46 cross entropy 0.00001 and Test error 1.410\n",
      "|| Epoch 47 cross entropy 0.00001 and Test error 1.410\n",
      "|| Epoch 48 cross entropy 0.00001 and Test error 1.410\n",
      "|| Epoch 49 cross entropy 0.00001 and Test error 1.410\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "for e in range(epochs):\n",
    "    ce, MC = 0.0, 0.0\n",
    "    for x, t in train_loader:  # sample one batch\n",
    "        x, t = x.view(-1, 784), t\n",
    "        predict = torch.mm(fRl(torch.mm(fRl(torch.mm(x, w1) + b1), w2) + b2), w3) + b3  # forward\n",
    "        o = Loss(predict, t)  # compute loss\n",
    "        o.backward()  # compute gradients\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        ce += o.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            x, t = x.view(-1, 784), t\n",
    "            predict = torch.mm(fRl(torch.mm(fRl(torch.mm(x, w1) + b1), w2) + b2), w3) + b3  # forward\n",
    "            index = torch.argmax(predict, 1)  # compute maximum\n",
    "            MC += ((index != t).float().sum()).item()  # accumulate error\n",
    "\n",
    "    print(f\"|| Epoch {e} cross entropy {ce/600:.5f} and Test error {100*MC/10000:.3f}\")\n",
    "\n",
    "# Aquí se entrena el modelo durante el número de épocas definido. En cada iteración, se hace una pasada hacia adelante, \n",
    "# se calcula la pérdida, se realiza la retropropagación para calcular los gradientes y se actualizan los parámetros.\n",
    "\n",
    "# Finalmente, el modelo se evalúa en el conjunto de prueba al final de cada época para calcular el error y la precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a696772-f637-459d-9a88-cebf744af9f6",
   "metadata": {},
   "source": [
    "* Epoch: Esta es la primera época de entrenamiento. Una época significa que el modelo ha pasado por todo el conjunto de datos de entrenamiento una vez.\n",
    "\n",
    "* Cross Entropy: 0.20941 es el valor de la pérdida de entropía cruzada después de la primera época. La entropía cruzada es una medida de la diferencia entre las predicciones del modelo y las etiquetas reales. Cuanto más bajo sea este número, mejor es el rendimiento del modelo en el conjunto de datos de entrenamiento.\n",
    "\n",
    "* Test Error: 3.070% es el error en el conjunto de prueba después de la primera época. Esto indica el porcentaje de predicciones incorrectas en el conjunto de datos de prueba.\n",
    "\n",
    "Este código entrena una red neuronal simple en el conjunto de datos MNIST. Pasa los datos a través de una red de tres capas, calcula la pérdida de entropía cruzada y ajusta los pesos usando el optimizador SGD. Después de cada época, el modelo se evalúa en el conjunto de datos de prueba para medir su rendimiento.\n",
    "\n",
    "Un valor más bajo de *entropía cruzada* indica que el modelo está haciendo predicciones más precisas en el conjunto de entrenamiento.\n",
    "El error en el *conjunto de prueba* también ha disminuido, lo que sugiere que el modelo está generalizando bien a los datos no vistos.\n",
    "\n",
    "### Análisis de resultados: \n",
    "\n",
    "Durante el entrenamiento del modelo de red neuronal en el conjunto de datos MNIST, se observaron resultados positivos a lo largo de 50 épocas. La pérdida de entropía cruzada, que mide la diferencia entre las predicciones y las etiquetas reales, disminuyó de 0.20941 en la época 0 a 0.00001 en la época 49. Esto indica que el modelo está aprendiendo eficazmente de los datos de entrenamiento.\n",
    "\n",
    "El error en el conjunto de prueba también mostró una disminución, pasando del 3.070% en la época 0 al 1.410% en la época 49. A pesar de ligeras fluctuaciones, el error se estabilizó alrededor del 1.4%, lo que sugiere que el modelo generaliza bien y mantiene una capacidad de predicción estable en datos no vistos.\n",
    "\n",
    "En resumen, el modelo está funcionando correctamente, aprendiendo eficazmente de los datos de entrenamiento y manteniendo un buen rendimiento en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31402f3d-eacc-425d-8954-d0178d171bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (deep_learning)",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
