{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hAiG8butgNj5"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb78IhJSgNj5"
      },
      "source": [
        "Word Embeddings: Encoding Lexical Semantics\n",
        "===========================================\n",
        "\n",
        "Word embeddings are dense vectors of real numbers, one per word in your\n",
        "vocabulary. In NLP, it is almost always the case that your features are\n",
        "words! But how should you represent a word in a computer? You could\n",
        "store its ascii character representation, but that only tells you what\n",
        "the word *is*, it doesn\\'t say much about what it *means* (you might be\n",
        "able to derive its part of speech from its affixes, or properties from\n",
        "its capitalization, but not much). Even more, in what sense could you\n",
        "combine these representations? We often want dense outputs from our\n",
        "neural networks, where the inputs are $|V|$ dimensional, where $V$ is\n",
        "our vocabulary, but often the outputs are only a few dimensional (if we\n",
        "are only predicting a handful of labels, for instance). How do we get\n",
        "from a massive dimensional space to a smaller dimensional space?\n",
        "\n",
        "How about instead of ascii representations, we use a one-hot encoding?\n",
        "That is, we represent the word $w$ by\n",
        "\n",
        "$$\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}$$\n",
        "\n",
        "where the 1 is in a location unique to $w$. Any other word will have a 1\n",
        "in some other location, and a 0 everywhere else.\n",
        "\n",
        "There is an enormous drawback to this representation, besides just how\n",
        "huge it is. It basically treats all words as independent entities with\n",
        "no relation to each other. What we really want is some notion of\n",
        "*similarity* between words. Why? Let\\'s see an example.\n",
        "\n",
        "Suppose we are building a language model. Suppose we have seen the\n",
        "sentences\n",
        "\n",
        "-   The mathematician ran to the store.\n",
        "-   The physicist ran to the store.\n",
        "-   The mathematician solved the open problem.\n",
        "\n",
        "in our training data. Now suppose we get a new sentence never before\n",
        "seen in our training data:\n",
        "\n",
        "-   The physicist solved the open problem.\n",
        "\n",
        "Our language model might do OK on this sentence, but wouldn\\'t it be\n",
        "much better if we could use the following two facts:\n",
        "\n",
        "-   We have seen mathematician and physicist in the same role in a\n",
        "    sentence. Somehow they have a semantic relation.\n",
        "-   We have seen mathematician in the same role in this new unseen\n",
        "    sentence as we are now seeing physicist.\n",
        "\n",
        "and then infer that physicist is actually a good fit in the new unseen\n",
        "sentence? This is what we mean by a notion of similarity: we mean\n",
        "*semantic similarity*, not simply having similar orthographic\n",
        "representations. It is a technique to combat the sparsity of linguistic\n",
        "data, by connecting the dots between what we have seen and what we\n",
        "haven\\'t. This example of course relies on a fundamental linguistic\n",
        "assumption: that words appearing in similar contexts are related to each\n",
        "other semantically. This is called the [distributional\n",
        "hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics).\n",
        "\n",
        "Getting Dense Word Embeddings\n",
        "-----------------------------\n",
        "\n",
        "How can we solve this problem? That is, how could we actually encode\n",
        "semantic similarity in words? Maybe we think up some semantic\n",
        "attributes. For example, we see that both mathematicians and physicists\n",
        "can run, so maybe we give these words a high score for the \\\"is able to\n",
        "run\\\" semantic attribute. Think of some other attributes, and imagine\n",
        "what you might score some common words on those attributes.\n",
        "\n",
        "If each attribute is a dimension, then we might give each word a vector,\n",
        "like this:\n",
        "\n",
        "$$q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n",
        "\\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]$$\n",
        "\n",
        "$$q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n",
        "\\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]$$\n",
        "\n",
        "Then we can get a measure of similarity between these words by doing:\n",
        "\n",
        "$$\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}$$\n",
        "\n",
        "Although it is more common to normalize by the lengths:\n",
        "\n",
        "$$\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}\n",
        "{\\| q_\\text{physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)$$\n",
        "\n",
        "Where $\\phi$ is the angle between the two vectors. That way, extremely\n",
        "similar words (words whose embeddings point in the same direction) will\n",
        "have similarity 1. Extremely dissimilar words should have similarity -1.\n",
        "\n",
        "You can think of the sparse one-hot vectors from the beginning of this\n",
        "section as a special case of these new vectors we have defined, where\n",
        "each word basically has similarity 0, and we gave each word some unique\n",
        "semantic attribute. These new vectors are *dense*, which is to say their\n",
        "entries are (typically) non-zero.\n",
        "\n",
        "But these new vectors are a big pain: you could think of thousands of\n",
        "different semantic attributes that might be relevant to determining\n",
        "similarity, and how on earth would you set the values of the different\n",
        "attributes? Central to the idea of deep learning is that the neural\n",
        "network learns representations of the features, rather than requiring\n",
        "the programmer to design them herself. So why not just let the word\n",
        "embeddings be parameters in our model, and then be updated during\n",
        "training? This is exactly what we will do. We will have some *latent\n",
        "semantic attributes* that the network can, in principle, learn. Note\n",
        "that the word embeddings will probably not be interpretable. That is,\n",
        "although with our hand-crafted vectors above we can see that\n",
        "mathematicians and physicists are similar in that they both like coffee,\n",
        "if we allow a neural network to learn the embeddings and see that both\n",
        "mathematicians and physicists have a large value in the second\n",
        "dimension, it is not clear what that means. They are similar in some\n",
        "latent semantic dimension, but this probably has no interpretation to\n",
        "us.\n",
        "\n",
        "In summary, **word embeddings are a representation of the \\*semantics\\*\n",
        "of a word, efficiently encoding semantic information that might be\n",
        "relevant to the task at hand**. You can embed other things too: part of\n",
        "speech tags, parse trees, anything! The idea of feature embeddings is\n",
        "central to the field.\n",
        "\n",
        "Word Embeddings in Pytorch\n",
        "--------------------------\n",
        "\n",
        "Before we get to a worked example and an exercise, a few quick notes\n",
        "about how to use embeddings in Pytorch and in deep learning programming\n",
        "in general. Similar to how we defined a unique index for each word when\n",
        "making one-hot vectors, we also need to define an index for each word\n",
        "when using embeddings. These will be keys into a lookup table. That is,\n",
        "embeddings are stored as a $|V| \\times D$ matrix, where $D$ is the\n",
        "dimensionality of the embeddings, such that the word assigned index $i$\n",
        "has its embedding stored in the $i$\\'th row of the matrix. In all of my\n",
        "code, the mapping from words to indices is a dictionary named\n",
        "word\\_to\\_ix.\n",
        "\n",
        "The module that allows you to use embeddings is torch.nn.Embedding,\n",
        "which takes two arguments: the vocabulary size, and the dimensionality\n",
        "of the embeddings.\n",
        "\n",
        "To index into this table, you must use torch.LongTensor (since the\n",
        "indices are integers, not floats).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ClZ2lPHgNj7",
        "outputId": "62ab91ca-99e2-402e-d9cb-52578fdaddde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c3438739550>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Author: Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6yewnLpgNj7",
        "outputId": "8b4c4652-db7f-4f62-80f5-82e7ffeaa64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
        "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jThiTJXTgNj7"
      },
      "source": [
        "An Example: N-Gram Language Modeling\n",
        "====================================\n",
        "\n",
        "Recall that in an n-gram language model, given a sequence of words $w$,\n",
        "we want to compute\n",
        "\n",
        "$$P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )$$\n",
        "\n",
        "Where $w_i$ is the ith word of the sequence.\n",
        "\n",
        "In this example, we will compute the loss function on some training\n",
        "examples and update the parameters with backpropagation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGLrSFi-gNj7",
        "outputId": "f05feec7-6ad4-4cb7-eee2-997ef985507c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n",
            "[521.625584602356, 519.0713486671448, 516.5341033935547, 514.0120182037354, 511.5031621456146, 509.007116317749, 506.5243458747864, 504.054993391037, 501.5971927642822, 499.1482117176056]\n",
            "tensor([-1.1796, -1.4428, -0.7074,  0.7528, -0.0203,  1.1036,  0.5368,  0.9796,\n",
            "        -1.1941, -0.4913], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "CONTEXT_SIZE = 2\n",
        "EMBEDDING_DIM = 10\n",
        "# We will use Shakespeare Sonnet 2\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
        "# we should tokenize the input, but we will ignore that for now\n",
        "# build a list of tuples.\n",
        "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
        "ngrams = [\n",
        "    (\n",
        "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
        "        test_sentence[i]\n",
        "    )\n",
        "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
        "]\n",
        "# Print the first 3, just so you can see what they look like.\n",
        "print(ngrams[:3])\n",
        "\n",
        "vocab = set(test_sentence)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for context, target in ngrams:\n",
        "\n",
        "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "        # into integer indices and wrap them in tensors)\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "        # new instance, you need to zero out the gradients from the old\n",
        "        # instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 3. Run the forward pass, getting log probabilities over next\n",
        "        # words\n",
        "        log_probs = model(context_idxs)\n",
        "\n",
        "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "        # word wrapped in a tensor)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "print(losses)  # The loss decreased every iteration over the training data!\n",
        "\n",
        "# To get the embedding of a particular word, e.g. \"beauty\"\n",
        "print(model.embeddings.weight[word_to_ix[\"beauty\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s4jReO1gNj8"
      },
      "source": [
        "Exercise: Computing Word Embeddings: Continuous Bag-of-Words\n",
        "============================================================\n",
        "\n",
        "The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep\n",
        "learning. It is a model that tries to predict words given the context of\n",
        "a few words before and a few words after the target word. This is\n",
        "distinct from language modeling, since CBOW is not sequential and does\n",
        "not have to be probabilistic. Typically, CBOW is used to quickly train\n",
        "word embeddings, and these embeddings are used to initialize the\n",
        "embeddings of some more complicated model. Usually, this is referred to\n",
        "as *pretraining embeddings*. It almost always helps performance a couple\n",
        "of percent.\n",
        "\n",
        "The CBOW model is as follows. Given a target word $w_i$ and an $N$\n",
        "context window on each side, $w_{i-1}, \\dots, w_{i-N}$ and\n",
        "$w_{i+1}, \\dots, w_{i+N}$, referring to all context words collectively\n",
        "as $C$, CBOW tries to minimize\n",
        "\n",
        "$$-\\log p(w_i | C) = -\\log \\text{Softmax}\\left(A(\\sum_{w \\in C} q_w) + b\\right)$$\n",
        "\n",
        "where $q_w$ is the embedding for word $w$.\n",
        "\n",
        "Implement this model in Pytorch by filling in the class below. Some\n",
        "tips:\n",
        "\n",
        "-   Think about which parameters you need to define.\n",
        "-   Make sure you know what shape each operation expects. Use .view() if\n",
        "    you need to reshape.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".lower().split()\n",
        "\n",
        "print(raw_text)"
      ],
      "metadata": {
        "id": "sGeSkew5mbbV",
        "outputId": "e7cf5210-0a92-49f9-9b0b-206c06cebf63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['we', 'are', 'about', 'to', 'study', 'the', 'idea', 'of', 'a', 'computational', 'process.', 'computational', 'processes', 'are', 'abstract', 'beings', 'that', 'inhabit', 'computers.', 'as', 'they', 'evolve,', 'processes', 'manipulate', 'other', 'abstract', 'things', 'called', 'data.', 'the', 'evolution', 'of', 'a', 'process', 'is', 'directed', 'by', 'a', 'pattern', 'of', 'rules', 'called', 'a', 'program.', 'people', 'create', 'programs', 'to', 'direct', 'processes.', 'in', 'effect,', 'we', 'conjure', 'the', 'spirits', 'of', 'the', 'computer', 'with', 'our', 'spells.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45T8uJUUgNj8",
        "outputId": "fcf7dbd8-a47c-4efd-b48d-bc76f867c6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['are', 'we', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study'), (['study', 'to', 'idea', 'of'], 'the'), (['the', 'study', 'of', 'a'], 'idea')]\n",
            "{'of', 'in', 'are', 'we', 'idea', 'rules', 'processes', 'beings', 'manipulate', 'direct', 'evolution', 'effect,', 'evolve,', 'conjure', 'by', 'to', 'about', 'that', 'program.', 'computational', 'people', 'programs', 'create', 'spells.', 'inhabit', 'a', 'is', 'things', 'processes.', 'computers.', 'as', 'process', 'computer', 'other', 'with', 'they', 'data.', 'abstract', 'the', 'pattern', 'study', 'process.', 'directed', 'spirits', 'called', 'our'}\n",
            "46\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2,  3, 15, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".lower().split()\n",
        "\n",
        "# By deriving a set from `raw_text`, we deduplicate the array\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "data = []\n",
        "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
        "    context = (\n",
        "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
        "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
        "    )\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "print(data[:5])\n",
        "\n",
        "\n",
        "print(vocab)\n",
        "print(vocab_size)\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        pass\n",
        "\n",
        "# Create your model and train. Here are some functions to help you make\n",
        "# the data ready for use by your module.\n",
        "\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "make_context_vector(data[0][0], word_to_ix)  # example"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "id": "iGflXldfnDh5",
        "outputId": "2579cbc7-3c67-42cf-9a02-54ad4900f351",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'of', 'in', 'are', 'we', 'idea', 'rules', 'processes', 'beings', 'manipulate', 'direct', 'evolution', 'effect,', 'evolve,', 'conjure', 'by', 'to', 'about', 'that', 'program.', 'computational', 'people', 'programs', 'create', 'spells.', 'inhabit', 'a', 'is', 'things', 'processes.', 'computers.', 'as', 'process', 'computer', 'other', 'with', 'they', 'data.', 'abstract', 'the', 'pattern', 'study', 'process.', 'directed', 'spirits', 'called', 'our'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "2nz9QhPdnHoe",
        "outputId": "0efb5a95-7c01-47ab-b014-8a4943ca2719",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Max iters\n",
        "\n",
        "max_iters=10\n",
        "\n",
        "for e in range (max_iters):\n",
        "  for x, t in data:\n",
        "    print(\"......\")\n",
        "    print(x)\n",
        "    print(t)"
      ],
      "metadata": {
        "id": "DxXNVIokn-o9",
        "outputId": "47f62fc2-d6cd-4b1d-a566-905ca9dffc1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "......\n",
            "['are', 'we', 'to', 'study']\n",
            "about\n",
            "......\n",
            "['about', 'are', 'study', 'the']\n",
            "to\n",
            "......\n",
            "['to', 'about', 'the', 'idea']\n",
            "study\n",
            "......\n",
            "['study', 'to', 'idea', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'study', 'of', 'a']\n",
            "idea\n",
            "......\n",
            "['idea', 'the', 'a', 'computational']\n",
            "of\n",
            "......\n",
            "['of', 'idea', 'computational', 'process.']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'process.', 'computational']\n",
            "computational\n",
            "......\n",
            "['computational', 'a', 'computational', 'processes']\n",
            "process.\n",
            "......\n",
            "['process.', 'computational', 'processes', 'are']\n",
            "computational\n",
            "......\n",
            "['computational', 'process.', 'are', 'abstract']\n",
            "processes\n",
            "......\n",
            "['processes', 'computational', 'abstract', 'beings']\n",
            "are\n",
            "......\n",
            "['are', 'processes', 'beings', 'that']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'are', 'that', 'inhabit']\n",
            "beings\n",
            "......\n",
            "['beings', 'abstract', 'inhabit', 'computers.']\n",
            "that\n",
            "......\n",
            "['that', 'beings', 'computers.', 'as']\n",
            "inhabit\n",
            "......\n",
            "['inhabit', 'that', 'as', 'they']\n",
            "computers.\n",
            "......\n",
            "['computers.', 'inhabit', 'they', 'evolve,']\n",
            "as\n",
            "......\n",
            "['as', 'computers.', 'evolve,', 'processes']\n",
            "they\n",
            "......\n",
            "['they', 'as', 'processes', 'manipulate']\n",
            "evolve,\n",
            "......\n",
            "['evolve,', 'they', 'manipulate', 'other']\n",
            "processes\n",
            "......\n",
            "['processes', 'evolve,', 'other', 'abstract']\n",
            "manipulate\n",
            "......\n",
            "['manipulate', 'processes', 'abstract', 'things']\n",
            "other\n",
            "......\n",
            "['other', 'manipulate', 'things', 'called']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'other', 'called', 'data.']\n",
            "things\n",
            "......\n",
            "['things', 'abstract', 'data.', 'the']\n",
            "called\n",
            "......\n",
            "['called', 'things', 'the', 'evolution']\n",
            "data.\n",
            "......\n",
            "['data.', 'called', 'evolution', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'data.', 'of', 'a']\n",
            "evolution\n",
            "......\n",
            "['evolution', 'the', 'a', 'process']\n",
            "of\n",
            "......\n",
            "['of', 'evolution', 'process', 'is']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'is', 'directed']\n",
            "process\n",
            "......\n",
            "['process', 'a', 'directed', 'by']\n",
            "is\n",
            "......\n",
            "['is', 'process', 'by', 'a']\n",
            "directed\n",
            "......\n",
            "['directed', 'is', 'a', 'pattern']\n",
            "by\n",
            "......\n",
            "['by', 'directed', 'pattern', 'of']\n",
            "a\n",
            "......\n",
            "['a', 'by', 'of', 'rules']\n",
            "pattern\n",
            "......\n",
            "['pattern', 'a', 'rules', 'called']\n",
            "of\n",
            "......\n",
            "['of', 'pattern', 'called', 'a']\n",
            "rules\n",
            "......\n",
            "['rules', 'of', 'a', 'program.']\n",
            "called\n",
            "......\n",
            "['called', 'rules', 'program.', 'people']\n",
            "a\n",
            "......\n",
            "['a', 'called', 'people', 'create']\n",
            "program.\n",
            "......\n",
            "['program.', 'a', 'create', 'programs']\n",
            "people\n",
            "......\n",
            "['people', 'program.', 'programs', 'to']\n",
            "create\n",
            "......\n",
            "['create', 'people', 'to', 'direct']\n",
            "programs\n",
            "......\n",
            "['programs', 'create', 'direct', 'processes.']\n",
            "to\n",
            "......\n",
            "['to', 'programs', 'processes.', 'in']\n",
            "direct\n",
            "......\n",
            "['direct', 'to', 'in', 'effect,']\n",
            "processes.\n",
            "......\n",
            "['processes.', 'direct', 'effect,', 'we']\n",
            "in\n",
            "......\n",
            "['in', 'processes.', 'we', 'conjure']\n",
            "effect,\n",
            "......\n",
            "['effect,', 'in', 'conjure', 'the']\n",
            "we\n",
            "......\n",
            "['we', 'effect,', 'the', 'spirits']\n",
            "conjure\n",
            "......\n",
            "['conjure', 'we', 'spirits', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'conjure', 'of', 'the']\n",
            "spirits\n",
            "......\n",
            "['spirits', 'the', 'the', 'computer']\n",
            "of\n",
            "......\n",
            "['of', 'spirits', 'computer', 'with']\n",
            "the\n",
            "......\n",
            "['the', 'of', 'with', 'our']\n",
            "computer\n",
            "......\n",
            "['computer', 'the', 'our', 'spells.']\n",
            "with\n",
            "......\n",
            "['are', 'we', 'to', 'study']\n",
            "about\n",
            "......\n",
            "['about', 'are', 'study', 'the']\n",
            "to\n",
            "......\n",
            "['to', 'about', 'the', 'idea']\n",
            "study\n",
            "......\n",
            "['study', 'to', 'idea', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'study', 'of', 'a']\n",
            "idea\n",
            "......\n",
            "['idea', 'the', 'a', 'computational']\n",
            "of\n",
            "......\n",
            "['of', 'idea', 'computational', 'process.']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'process.', 'computational']\n",
            "computational\n",
            "......\n",
            "['computational', 'a', 'computational', 'processes']\n",
            "process.\n",
            "......\n",
            "['process.', 'computational', 'processes', 'are']\n",
            "computational\n",
            "......\n",
            "['computational', 'process.', 'are', 'abstract']\n",
            "processes\n",
            "......\n",
            "['processes', 'computational', 'abstract', 'beings']\n",
            "are\n",
            "......\n",
            "['are', 'processes', 'beings', 'that']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'are', 'that', 'inhabit']\n",
            "beings\n",
            "......\n",
            "['beings', 'abstract', 'inhabit', 'computers.']\n",
            "that\n",
            "......\n",
            "['that', 'beings', 'computers.', 'as']\n",
            "inhabit\n",
            "......\n",
            "['inhabit', 'that', 'as', 'they']\n",
            "computers.\n",
            "......\n",
            "['computers.', 'inhabit', 'they', 'evolve,']\n",
            "as\n",
            "......\n",
            "['as', 'computers.', 'evolve,', 'processes']\n",
            "they\n",
            "......\n",
            "['they', 'as', 'processes', 'manipulate']\n",
            "evolve,\n",
            "......\n",
            "['evolve,', 'they', 'manipulate', 'other']\n",
            "processes\n",
            "......\n",
            "['processes', 'evolve,', 'other', 'abstract']\n",
            "manipulate\n",
            "......\n",
            "['manipulate', 'processes', 'abstract', 'things']\n",
            "other\n",
            "......\n",
            "['other', 'manipulate', 'things', 'called']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'other', 'called', 'data.']\n",
            "things\n",
            "......\n",
            "['things', 'abstract', 'data.', 'the']\n",
            "called\n",
            "......\n",
            "['called', 'things', 'the', 'evolution']\n",
            "data.\n",
            "......\n",
            "['data.', 'called', 'evolution', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'data.', 'of', 'a']\n",
            "evolution\n",
            "......\n",
            "['evolution', 'the', 'a', 'process']\n",
            "of\n",
            "......\n",
            "['of', 'evolution', 'process', 'is']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'is', 'directed']\n",
            "process\n",
            "......\n",
            "['process', 'a', 'directed', 'by']\n",
            "is\n",
            "......\n",
            "['is', 'process', 'by', 'a']\n",
            "directed\n",
            "......\n",
            "['directed', 'is', 'a', 'pattern']\n",
            "by\n",
            "......\n",
            "['by', 'directed', 'pattern', 'of']\n",
            "a\n",
            "......\n",
            "['a', 'by', 'of', 'rules']\n",
            "pattern\n",
            "......\n",
            "['pattern', 'a', 'rules', 'called']\n",
            "of\n",
            "......\n",
            "['of', 'pattern', 'called', 'a']\n",
            "rules\n",
            "......\n",
            "['rules', 'of', 'a', 'program.']\n",
            "called\n",
            "......\n",
            "['called', 'rules', 'program.', 'people']\n",
            "a\n",
            "......\n",
            "['a', 'called', 'people', 'create']\n",
            "program.\n",
            "......\n",
            "['program.', 'a', 'create', 'programs']\n",
            "people\n",
            "......\n",
            "['people', 'program.', 'programs', 'to']\n",
            "create\n",
            "......\n",
            "['create', 'people', 'to', 'direct']\n",
            "programs\n",
            "......\n",
            "['programs', 'create', 'direct', 'processes.']\n",
            "to\n",
            "......\n",
            "['to', 'programs', 'processes.', 'in']\n",
            "direct\n",
            "......\n",
            "['direct', 'to', 'in', 'effect,']\n",
            "processes.\n",
            "......\n",
            "['processes.', 'direct', 'effect,', 'we']\n",
            "in\n",
            "......\n",
            "['in', 'processes.', 'we', 'conjure']\n",
            "effect,\n",
            "......\n",
            "['effect,', 'in', 'conjure', 'the']\n",
            "we\n",
            "......\n",
            "['we', 'effect,', 'the', 'spirits']\n",
            "conjure\n",
            "......\n",
            "['conjure', 'we', 'spirits', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'conjure', 'of', 'the']\n",
            "spirits\n",
            "......\n",
            "['spirits', 'the', 'the', 'computer']\n",
            "of\n",
            "......\n",
            "['of', 'spirits', 'computer', 'with']\n",
            "the\n",
            "......\n",
            "['the', 'of', 'with', 'our']\n",
            "computer\n",
            "......\n",
            "['computer', 'the', 'our', 'spells.']\n",
            "with\n",
            "......\n",
            "['are', 'we', 'to', 'study']\n",
            "about\n",
            "......\n",
            "['about', 'are', 'study', 'the']\n",
            "to\n",
            "......\n",
            "['to', 'about', 'the', 'idea']\n",
            "study\n",
            "......\n",
            "['study', 'to', 'idea', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'study', 'of', 'a']\n",
            "idea\n",
            "......\n",
            "['idea', 'the', 'a', 'computational']\n",
            "of\n",
            "......\n",
            "['of', 'idea', 'computational', 'process.']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'process.', 'computational']\n",
            "computational\n",
            "......\n",
            "['computational', 'a', 'computational', 'processes']\n",
            "process.\n",
            "......\n",
            "['process.', 'computational', 'processes', 'are']\n",
            "computational\n",
            "......\n",
            "['computational', 'process.', 'are', 'abstract']\n",
            "processes\n",
            "......\n",
            "['processes', 'computational', 'abstract', 'beings']\n",
            "are\n",
            "......\n",
            "['are', 'processes', 'beings', 'that']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'are', 'that', 'inhabit']\n",
            "beings\n",
            "......\n",
            "['beings', 'abstract', 'inhabit', 'computers.']\n",
            "that\n",
            "......\n",
            "['that', 'beings', 'computers.', 'as']\n",
            "inhabit\n",
            "......\n",
            "['inhabit', 'that', 'as', 'they']\n",
            "computers.\n",
            "......\n",
            "['computers.', 'inhabit', 'they', 'evolve,']\n",
            "as\n",
            "......\n",
            "['as', 'computers.', 'evolve,', 'processes']\n",
            "they\n",
            "......\n",
            "['they', 'as', 'processes', 'manipulate']\n",
            "evolve,\n",
            "......\n",
            "['evolve,', 'they', 'manipulate', 'other']\n",
            "processes\n",
            "......\n",
            "['processes', 'evolve,', 'other', 'abstract']\n",
            "manipulate\n",
            "......\n",
            "['manipulate', 'processes', 'abstract', 'things']\n",
            "other\n",
            "......\n",
            "['other', 'manipulate', 'things', 'called']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'other', 'called', 'data.']\n",
            "things\n",
            "......\n",
            "['things', 'abstract', 'data.', 'the']\n",
            "called\n",
            "......\n",
            "['called', 'things', 'the', 'evolution']\n",
            "data.\n",
            "......\n",
            "['data.', 'called', 'evolution', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'data.', 'of', 'a']\n",
            "evolution\n",
            "......\n",
            "['evolution', 'the', 'a', 'process']\n",
            "of\n",
            "......\n",
            "['of', 'evolution', 'process', 'is']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'is', 'directed']\n",
            "process\n",
            "......\n",
            "['process', 'a', 'directed', 'by']\n",
            "is\n",
            "......\n",
            "['is', 'process', 'by', 'a']\n",
            "directed\n",
            "......\n",
            "['directed', 'is', 'a', 'pattern']\n",
            "by\n",
            "......\n",
            "['by', 'directed', 'pattern', 'of']\n",
            "a\n",
            "......\n",
            "['a', 'by', 'of', 'rules']\n",
            "pattern\n",
            "......\n",
            "['pattern', 'a', 'rules', 'called']\n",
            "of\n",
            "......\n",
            "['of', 'pattern', 'called', 'a']\n",
            "rules\n",
            "......\n",
            "['rules', 'of', 'a', 'program.']\n",
            "called\n",
            "......\n",
            "['called', 'rules', 'program.', 'people']\n",
            "a\n",
            "......\n",
            "['a', 'called', 'people', 'create']\n",
            "program.\n",
            "......\n",
            "['program.', 'a', 'create', 'programs']\n",
            "people\n",
            "......\n",
            "['people', 'program.', 'programs', 'to']\n",
            "create\n",
            "......\n",
            "['create', 'people', 'to', 'direct']\n",
            "programs\n",
            "......\n",
            "['programs', 'create', 'direct', 'processes.']\n",
            "to\n",
            "......\n",
            "['to', 'programs', 'processes.', 'in']\n",
            "direct\n",
            "......\n",
            "['direct', 'to', 'in', 'effect,']\n",
            "processes.\n",
            "......\n",
            "['processes.', 'direct', 'effect,', 'we']\n",
            "in\n",
            "......\n",
            "['in', 'processes.', 'we', 'conjure']\n",
            "effect,\n",
            "......\n",
            "['effect,', 'in', 'conjure', 'the']\n",
            "we\n",
            "......\n",
            "['we', 'effect,', 'the', 'spirits']\n",
            "conjure\n",
            "......\n",
            "['conjure', 'we', 'spirits', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'conjure', 'of', 'the']\n",
            "spirits\n",
            "......\n",
            "['spirits', 'the', 'the', 'computer']\n",
            "of\n",
            "......\n",
            "['of', 'spirits', 'computer', 'with']\n",
            "the\n",
            "......\n",
            "['the', 'of', 'with', 'our']\n",
            "computer\n",
            "......\n",
            "['computer', 'the', 'our', 'spells.']\n",
            "with\n",
            "......\n",
            "['are', 'we', 'to', 'study']\n",
            "about\n",
            "......\n",
            "['about', 'are', 'study', 'the']\n",
            "to\n",
            "......\n",
            "['to', 'about', 'the', 'idea']\n",
            "study\n",
            "......\n",
            "['study', 'to', 'idea', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'study', 'of', 'a']\n",
            "idea\n",
            "......\n",
            "['idea', 'the', 'a', 'computational']\n",
            "of\n",
            "......\n",
            "['of', 'idea', 'computational', 'process.']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'process.', 'computational']\n",
            "computational\n",
            "......\n",
            "['computational', 'a', 'computational', 'processes']\n",
            "process.\n",
            "......\n",
            "['process.', 'computational', 'processes', 'are']\n",
            "computational\n",
            "......\n",
            "['computational', 'process.', 'are', 'abstract']\n",
            "processes\n",
            "......\n",
            "['processes', 'computational', 'abstract', 'beings']\n",
            "are\n",
            "......\n",
            "['are', 'processes', 'beings', 'that']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'are', 'that', 'inhabit']\n",
            "beings\n",
            "......\n",
            "['beings', 'abstract', 'inhabit', 'computers.']\n",
            "that\n",
            "......\n",
            "['that', 'beings', 'computers.', 'as']\n",
            "inhabit\n",
            "......\n",
            "['inhabit', 'that', 'as', 'they']\n",
            "computers.\n",
            "......\n",
            "['computers.', 'inhabit', 'they', 'evolve,']\n",
            "as\n",
            "......\n",
            "['as', 'computers.', 'evolve,', 'processes']\n",
            "they\n",
            "......\n",
            "['they', 'as', 'processes', 'manipulate']\n",
            "evolve,\n",
            "......\n",
            "['evolve,', 'they', 'manipulate', 'other']\n",
            "processes\n",
            "......\n",
            "['processes', 'evolve,', 'other', 'abstract']\n",
            "manipulate\n",
            "......\n",
            "['manipulate', 'processes', 'abstract', 'things']\n",
            "other\n",
            "......\n",
            "['other', 'manipulate', 'things', 'called']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'other', 'called', 'data.']\n",
            "things\n",
            "......\n",
            "['things', 'abstract', 'data.', 'the']\n",
            "called\n",
            "......\n",
            "['called', 'things', 'the', 'evolution']\n",
            "data.\n",
            "......\n",
            "['data.', 'called', 'evolution', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'data.', 'of', 'a']\n",
            "evolution\n",
            "......\n",
            "['evolution', 'the', 'a', 'process']\n",
            "of\n",
            "......\n",
            "['of', 'evolution', 'process', 'is']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'is', 'directed']\n",
            "process\n",
            "......\n",
            "['process', 'a', 'directed', 'by']\n",
            "is\n",
            "......\n",
            "['is', 'process', 'by', 'a']\n",
            "directed\n",
            "......\n",
            "['directed', 'is', 'a', 'pattern']\n",
            "by\n",
            "......\n",
            "['by', 'directed', 'pattern', 'of']\n",
            "a\n",
            "......\n",
            "['a', 'by', 'of', 'rules']\n",
            "pattern\n",
            "......\n",
            "['pattern', 'a', 'rules', 'called']\n",
            "of\n",
            "......\n",
            "['of', 'pattern', 'called', 'a']\n",
            "rules\n",
            "......\n",
            "['rules', 'of', 'a', 'program.']\n",
            "called\n",
            "......\n",
            "['called', 'rules', 'program.', 'people']\n",
            "a\n",
            "......\n",
            "['a', 'called', 'people', 'create']\n",
            "program.\n",
            "......\n",
            "['program.', 'a', 'create', 'programs']\n",
            "people\n",
            "......\n",
            "['people', 'program.', 'programs', 'to']\n",
            "create\n",
            "......\n",
            "['create', 'people', 'to', 'direct']\n",
            "programs\n",
            "......\n",
            "['programs', 'create', 'direct', 'processes.']\n",
            "to\n",
            "......\n",
            "['to', 'programs', 'processes.', 'in']\n",
            "direct\n",
            "......\n",
            "['direct', 'to', 'in', 'effect,']\n",
            "processes.\n",
            "......\n",
            "['processes.', 'direct', 'effect,', 'we']\n",
            "in\n",
            "......\n",
            "['in', 'processes.', 'we', 'conjure']\n",
            "effect,\n",
            "......\n",
            "['effect,', 'in', 'conjure', 'the']\n",
            "we\n",
            "......\n",
            "['we', 'effect,', 'the', 'spirits']\n",
            "conjure\n",
            "......\n",
            "['conjure', 'we', 'spirits', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'conjure', 'of', 'the']\n",
            "spirits\n",
            "......\n",
            "['spirits', 'the', 'the', 'computer']\n",
            "of\n",
            "......\n",
            "['of', 'spirits', 'computer', 'with']\n",
            "the\n",
            "......\n",
            "['the', 'of', 'with', 'our']\n",
            "computer\n",
            "......\n",
            "['computer', 'the', 'our', 'spells.']\n",
            "with\n",
            "......\n",
            "['are', 'we', 'to', 'study']\n",
            "about\n",
            "......\n",
            "['about', 'are', 'study', 'the']\n",
            "to\n",
            "......\n",
            "['to', 'about', 'the', 'idea']\n",
            "study\n",
            "......\n",
            "['study', 'to', 'idea', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'study', 'of', 'a']\n",
            "idea\n",
            "......\n",
            "['idea', 'the', 'a', 'computational']\n",
            "of\n",
            "......\n",
            "['of', 'idea', 'computational', 'process.']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'process.', 'computational']\n",
            "computational\n",
            "......\n",
            "['computational', 'a', 'computational', 'processes']\n",
            "process.\n",
            "......\n",
            "['process.', 'computational', 'processes', 'are']\n",
            "computational\n",
            "......\n",
            "['computational', 'process.', 'are', 'abstract']\n",
            "processes\n",
            "......\n",
            "['processes', 'computational', 'abstract', 'beings']\n",
            "are\n",
            "......\n",
            "['are', 'processes', 'beings', 'that']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'are', 'that', 'inhabit']\n",
            "beings\n",
            "......\n",
            "['beings', 'abstract', 'inhabit', 'computers.']\n",
            "that\n",
            "......\n",
            "['that', 'beings', 'computers.', 'as']\n",
            "inhabit\n",
            "......\n",
            "['inhabit', 'that', 'as', 'they']\n",
            "computers.\n",
            "......\n",
            "['computers.', 'inhabit', 'they', 'evolve,']\n",
            "as\n",
            "......\n",
            "['as', 'computers.', 'evolve,', 'processes']\n",
            "they\n",
            "......\n",
            "['they', 'as', 'processes', 'manipulate']\n",
            "evolve,\n",
            "......\n",
            "['evolve,', 'they', 'manipulate', 'other']\n",
            "processes\n",
            "......\n",
            "['processes', 'evolve,', 'other', 'abstract']\n",
            "manipulate\n",
            "......\n",
            "['manipulate', 'processes', 'abstract', 'things']\n",
            "other\n",
            "......\n",
            "['other', 'manipulate', 'things', 'called']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'other', 'called', 'data.']\n",
            "things\n",
            "......\n",
            "['things', 'abstract', 'data.', 'the']\n",
            "called\n",
            "......\n",
            "['called', 'things', 'the', 'evolution']\n",
            "data.\n",
            "......\n",
            "['data.', 'called', 'evolution', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'data.', 'of', 'a']\n",
            "evolution\n",
            "......\n",
            "['evolution', 'the', 'a', 'process']\n",
            "of\n",
            "......\n",
            "['of', 'evolution', 'process', 'is']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'is', 'directed']\n",
            "process\n",
            "......\n",
            "['process', 'a', 'directed', 'by']\n",
            "is\n",
            "......\n",
            "['is', 'process', 'by', 'a']\n",
            "directed\n",
            "......\n",
            "['directed', 'is', 'a', 'pattern']\n",
            "by\n",
            "......\n",
            "['by', 'directed', 'pattern', 'of']\n",
            "a\n",
            "......\n",
            "['a', 'by', 'of', 'rules']\n",
            "pattern\n",
            "......\n",
            "['pattern', 'a', 'rules', 'called']\n",
            "of\n",
            "......\n",
            "['of', 'pattern', 'called', 'a']\n",
            "rules\n",
            "......\n",
            "['rules', 'of', 'a', 'program.']\n",
            "called\n",
            "......\n",
            "['called', 'rules', 'program.', 'people']\n",
            "a\n",
            "......\n",
            "['a', 'called', 'people', 'create']\n",
            "program.\n",
            "......\n",
            "['program.', 'a', 'create', 'programs']\n",
            "people\n",
            "......\n",
            "['people', 'program.', 'programs', 'to']\n",
            "create\n",
            "......\n",
            "['create', 'people', 'to', 'direct']\n",
            "programs\n",
            "......\n",
            "['programs', 'create', 'direct', 'processes.']\n",
            "to\n",
            "......\n",
            "['to', 'programs', 'processes.', 'in']\n",
            "direct\n",
            "......\n",
            "['direct', 'to', 'in', 'effect,']\n",
            "processes.\n",
            "......\n",
            "['processes.', 'direct', 'effect,', 'we']\n",
            "in\n",
            "......\n",
            "['in', 'processes.', 'we', 'conjure']\n",
            "effect,\n",
            "......\n",
            "['effect,', 'in', 'conjure', 'the']\n",
            "we\n",
            "......\n",
            "['we', 'effect,', 'the', 'spirits']\n",
            "conjure\n",
            "......\n",
            "['conjure', 'we', 'spirits', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'conjure', 'of', 'the']\n",
            "spirits\n",
            "......\n",
            "['spirits', 'the', 'the', 'computer']\n",
            "of\n",
            "......\n",
            "['of', 'spirits', 'computer', 'with']\n",
            "the\n",
            "......\n",
            "['the', 'of', 'with', 'our']\n",
            "computer\n",
            "......\n",
            "['computer', 'the', 'our', 'spells.']\n",
            "with\n",
            "......\n",
            "['are', 'we', 'to', 'study']\n",
            "about\n",
            "......\n",
            "['about', 'are', 'study', 'the']\n",
            "to\n",
            "......\n",
            "['to', 'about', 'the', 'idea']\n",
            "study\n",
            "......\n",
            "['study', 'to', 'idea', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'study', 'of', 'a']\n",
            "idea\n",
            "......\n",
            "['idea', 'the', 'a', 'computational']\n",
            "of\n",
            "......\n",
            "['of', 'idea', 'computational', 'process.']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'process.', 'computational']\n",
            "computational\n",
            "......\n",
            "['computational', 'a', 'computational', 'processes']\n",
            "process.\n",
            "......\n",
            "['process.', 'computational', 'processes', 'are']\n",
            "computational\n",
            "......\n",
            "['computational', 'process.', 'are', 'abstract']\n",
            "processes\n",
            "......\n",
            "['processes', 'computational', 'abstract', 'beings']\n",
            "are\n",
            "......\n",
            "['are', 'processes', 'beings', 'that']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'are', 'that', 'inhabit']\n",
            "beings\n",
            "......\n",
            "['beings', 'abstract', 'inhabit', 'computers.']\n",
            "that\n",
            "......\n",
            "['that', 'beings', 'computers.', 'as']\n",
            "inhabit\n",
            "......\n",
            "['inhabit', 'that', 'as', 'they']\n",
            "computers.\n",
            "......\n",
            "['computers.', 'inhabit', 'they', 'evolve,']\n",
            "as\n",
            "......\n",
            "['as', 'computers.', 'evolve,', 'processes']\n",
            "they\n",
            "......\n",
            "['they', 'as', 'processes', 'manipulate']\n",
            "evolve,\n",
            "......\n",
            "['evolve,', 'they', 'manipulate', 'other']\n",
            "processes\n",
            "......\n",
            "['processes', 'evolve,', 'other', 'abstract']\n",
            "manipulate\n",
            "......\n",
            "['manipulate', 'processes', 'abstract', 'things']\n",
            "other\n",
            "......\n",
            "['other', 'manipulate', 'things', 'called']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'other', 'called', 'data.']\n",
            "things\n",
            "......\n",
            "['things', 'abstract', 'data.', 'the']\n",
            "called\n",
            "......\n",
            "['called', 'things', 'the', 'evolution']\n",
            "data.\n",
            "......\n",
            "['data.', 'called', 'evolution', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'data.', 'of', 'a']\n",
            "evolution\n",
            "......\n",
            "['evolution', 'the', 'a', 'process']\n",
            "of\n",
            "......\n",
            "['of', 'evolution', 'process', 'is']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'is', 'directed']\n",
            "process\n",
            "......\n",
            "['process', 'a', 'directed', 'by']\n",
            "is\n",
            "......\n",
            "['is', 'process', 'by', 'a']\n",
            "directed\n",
            "......\n",
            "['directed', 'is', 'a', 'pattern']\n",
            "by\n",
            "......\n",
            "['by', 'directed', 'pattern', 'of']\n",
            "a\n",
            "......\n",
            "['a', 'by', 'of', 'rules']\n",
            "pattern\n",
            "......\n",
            "['pattern', 'a', 'rules', 'called']\n",
            "of\n",
            "......\n",
            "['of', 'pattern', 'called', 'a']\n",
            "rules\n",
            "......\n",
            "['rules', 'of', 'a', 'program.']\n",
            "called\n",
            "......\n",
            "['called', 'rules', 'program.', 'people']\n",
            "a\n",
            "......\n",
            "['a', 'called', 'people', 'create']\n",
            "program.\n",
            "......\n",
            "['program.', 'a', 'create', 'programs']\n",
            "people\n",
            "......\n",
            "['people', 'program.', 'programs', 'to']\n",
            "create\n",
            "......\n",
            "['create', 'people', 'to', 'direct']\n",
            "programs\n",
            "......\n",
            "['programs', 'create', 'direct', 'processes.']\n",
            "to\n",
            "......\n",
            "['to', 'programs', 'processes.', 'in']\n",
            "direct\n",
            "......\n",
            "['direct', 'to', 'in', 'effect,']\n",
            "processes.\n",
            "......\n",
            "['processes.', 'direct', 'effect,', 'we']\n",
            "in\n",
            "......\n",
            "['in', 'processes.', 'we', 'conjure']\n",
            "effect,\n",
            "......\n",
            "['effect,', 'in', 'conjure', 'the']\n",
            "we\n",
            "......\n",
            "['we', 'effect,', 'the', 'spirits']\n",
            "conjure\n",
            "......\n",
            "['conjure', 'we', 'spirits', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'conjure', 'of', 'the']\n",
            "spirits\n",
            "......\n",
            "['spirits', 'the', 'the', 'computer']\n",
            "of\n",
            "......\n",
            "['of', 'spirits', 'computer', 'with']\n",
            "the\n",
            "......\n",
            "['the', 'of', 'with', 'our']\n",
            "computer\n",
            "......\n",
            "['computer', 'the', 'our', 'spells.']\n",
            "with\n",
            "......\n",
            "['are', 'we', 'to', 'study']\n",
            "about\n",
            "......\n",
            "['about', 'are', 'study', 'the']\n",
            "to\n",
            "......\n",
            "['to', 'about', 'the', 'idea']\n",
            "study\n",
            "......\n",
            "['study', 'to', 'idea', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'study', 'of', 'a']\n",
            "idea\n",
            "......\n",
            "['idea', 'the', 'a', 'computational']\n",
            "of\n",
            "......\n",
            "['of', 'idea', 'computational', 'process.']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'process.', 'computational']\n",
            "computational\n",
            "......\n",
            "['computational', 'a', 'computational', 'processes']\n",
            "process.\n",
            "......\n",
            "['process.', 'computational', 'processes', 'are']\n",
            "computational\n",
            "......\n",
            "['computational', 'process.', 'are', 'abstract']\n",
            "processes\n",
            "......\n",
            "['processes', 'computational', 'abstract', 'beings']\n",
            "are\n",
            "......\n",
            "['are', 'processes', 'beings', 'that']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'are', 'that', 'inhabit']\n",
            "beings\n",
            "......\n",
            "['beings', 'abstract', 'inhabit', 'computers.']\n",
            "that\n",
            "......\n",
            "['that', 'beings', 'computers.', 'as']\n",
            "inhabit\n",
            "......\n",
            "['inhabit', 'that', 'as', 'they']\n",
            "computers.\n",
            "......\n",
            "['computers.', 'inhabit', 'they', 'evolve,']\n",
            "as\n",
            "......\n",
            "['as', 'computers.', 'evolve,', 'processes']\n",
            "they\n",
            "......\n",
            "['they', 'as', 'processes', 'manipulate']\n",
            "evolve,\n",
            "......\n",
            "['evolve,', 'they', 'manipulate', 'other']\n",
            "processes\n",
            "......\n",
            "['processes', 'evolve,', 'other', 'abstract']\n",
            "manipulate\n",
            "......\n",
            "['manipulate', 'processes', 'abstract', 'things']\n",
            "other\n",
            "......\n",
            "['other', 'manipulate', 'things', 'called']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'other', 'called', 'data.']\n",
            "things\n",
            "......\n",
            "['things', 'abstract', 'data.', 'the']\n",
            "called\n",
            "......\n",
            "['called', 'things', 'the', 'evolution']\n",
            "data.\n",
            "......\n",
            "['data.', 'called', 'evolution', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'data.', 'of', 'a']\n",
            "evolution\n",
            "......\n",
            "['evolution', 'the', 'a', 'process']\n",
            "of\n",
            "......\n",
            "['of', 'evolution', 'process', 'is']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'is', 'directed']\n",
            "process\n",
            "......\n",
            "['process', 'a', 'directed', 'by']\n",
            "is\n",
            "......\n",
            "['is', 'process', 'by', 'a']\n",
            "directed\n",
            "......\n",
            "['directed', 'is', 'a', 'pattern']\n",
            "by\n",
            "......\n",
            "['by', 'directed', 'pattern', 'of']\n",
            "a\n",
            "......\n",
            "['a', 'by', 'of', 'rules']\n",
            "pattern\n",
            "......\n",
            "['pattern', 'a', 'rules', 'called']\n",
            "of\n",
            "......\n",
            "['of', 'pattern', 'called', 'a']\n",
            "rules\n",
            "......\n",
            "['rules', 'of', 'a', 'program.']\n",
            "called\n",
            "......\n",
            "['called', 'rules', 'program.', 'people']\n",
            "a\n",
            "......\n",
            "['a', 'called', 'people', 'create']\n",
            "program.\n",
            "......\n",
            "['program.', 'a', 'create', 'programs']\n",
            "people\n",
            "......\n",
            "['people', 'program.', 'programs', 'to']\n",
            "create\n",
            "......\n",
            "['create', 'people', 'to', 'direct']\n",
            "programs\n",
            "......\n",
            "['programs', 'create', 'direct', 'processes.']\n",
            "to\n",
            "......\n",
            "['to', 'programs', 'processes.', 'in']\n",
            "direct\n",
            "......\n",
            "['direct', 'to', 'in', 'effect,']\n",
            "processes.\n",
            "......\n",
            "['processes.', 'direct', 'effect,', 'we']\n",
            "in\n",
            "......\n",
            "['in', 'processes.', 'we', 'conjure']\n",
            "effect,\n",
            "......\n",
            "['effect,', 'in', 'conjure', 'the']\n",
            "we\n",
            "......\n",
            "['we', 'effect,', 'the', 'spirits']\n",
            "conjure\n",
            "......\n",
            "['conjure', 'we', 'spirits', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'conjure', 'of', 'the']\n",
            "spirits\n",
            "......\n",
            "['spirits', 'the', 'the', 'computer']\n",
            "of\n",
            "......\n",
            "['of', 'spirits', 'computer', 'with']\n",
            "the\n",
            "......\n",
            "['the', 'of', 'with', 'our']\n",
            "computer\n",
            "......\n",
            "['computer', 'the', 'our', 'spells.']\n",
            "with\n",
            "......\n",
            "['are', 'we', 'to', 'study']\n",
            "about\n",
            "......\n",
            "['about', 'are', 'study', 'the']\n",
            "to\n",
            "......\n",
            "['to', 'about', 'the', 'idea']\n",
            "study\n",
            "......\n",
            "['study', 'to', 'idea', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'study', 'of', 'a']\n",
            "idea\n",
            "......\n",
            "['idea', 'the', 'a', 'computational']\n",
            "of\n",
            "......\n",
            "['of', 'idea', 'computational', 'process.']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'process.', 'computational']\n",
            "computational\n",
            "......\n",
            "['computational', 'a', 'computational', 'processes']\n",
            "process.\n",
            "......\n",
            "['process.', 'computational', 'processes', 'are']\n",
            "computational\n",
            "......\n",
            "['computational', 'process.', 'are', 'abstract']\n",
            "processes\n",
            "......\n",
            "['processes', 'computational', 'abstract', 'beings']\n",
            "are\n",
            "......\n",
            "['are', 'processes', 'beings', 'that']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'are', 'that', 'inhabit']\n",
            "beings\n",
            "......\n",
            "['beings', 'abstract', 'inhabit', 'computers.']\n",
            "that\n",
            "......\n",
            "['that', 'beings', 'computers.', 'as']\n",
            "inhabit\n",
            "......\n",
            "['inhabit', 'that', 'as', 'they']\n",
            "computers.\n",
            "......\n",
            "['computers.', 'inhabit', 'they', 'evolve,']\n",
            "as\n",
            "......\n",
            "['as', 'computers.', 'evolve,', 'processes']\n",
            "they\n",
            "......\n",
            "['they', 'as', 'processes', 'manipulate']\n",
            "evolve,\n",
            "......\n",
            "['evolve,', 'they', 'manipulate', 'other']\n",
            "processes\n",
            "......\n",
            "['processes', 'evolve,', 'other', 'abstract']\n",
            "manipulate\n",
            "......\n",
            "['manipulate', 'processes', 'abstract', 'things']\n",
            "other\n",
            "......\n",
            "['other', 'manipulate', 'things', 'called']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'other', 'called', 'data.']\n",
            "things\n",
            "......\n",
            "['things', 'abstract', 'data.', 'the']\n",
            "called\n",
            "......\n",
            "['called', 'things', 'the', 'evolution']\n",
            "data.\n",
            "......\n",
            "['data.', 'called', 'evolution', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'data.', 'of', 'a']\n",
            "evolution\n",
            "......\n",
            "['evolution', 'the', 'a', 'process']\n",
            "of\n",
            "......\n",
            "['of', 'evolution', 'process', 'is']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'is', 'directed']\n",
            "process\n",
            "......\n",
            "['process', 'a', 'directed', 'by']\n",
            "is\n",
            "......\n",
            "['is', 'process', 'by', 'a']\n",
            "directed\n",
            "......\n",
            "['directed', 'is', 'a', 'pattern']\n",
            "by\n",
            "......\n",
            "['by', 'directed', 'pattern', 'of']\n",
            "a\n",
            "......\n",
            "['a', 'by', 'of', 'rules']\n",
            "pattern\n",
            "......\n",
            "['pattern', 'a', 'rules', 'called']\n",
            "of\n",
            "......\n",
            "['of', 'pattern', 'called', 'a']\n",
            "rules\n",
            "......\n",
            "['rules', 'of', 'a', 'program.']\n",
            "called\n",
            "......\n",
            "['called', 'rules', 'program.', 'people']\n",
            "a\n",
            "......\n",
            "['a', 'called', 'people', 'create']\n",
            "program.\n",
            "......\n",
            "['program.', 'a', 'create', 'programs']\n",
            "people\n",
            "......\n",
            "['people', 'program.', 'programs', 'to']\n",
            "create\n",
            "......\n",
            "['create', 'people', 'to', 'direct']\n",
            "programs\n",
            "......\n",
            "['programs', 'create', 'direct', 'processes.']\n",
            "to\n",
            "......\n",
            "['to', 'programs', 'processes.', 'in']\n",
            "direct\n",
            "......\n",
            "['direct', 'to', 'in', 'effect,']\n",
            "processes.\n",
            "......\n",
            "['processes.', 'direct', 'effect,', 'we']\n",
            "in\n",
            "......\n",
            "['in', 'processes.', 'we', 'conjure']\n",
            "effect,\n",
            "......\n",
            "['effect,', 'in', 'conjure', 'the']\n",
            "we\n",
            "......\n",
            "['we', 'effect,', 'the', 'spirits']\n",
            "conjure\n",
            "......\n",
            "['conjure', 'we', 'spirits', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'conjure', 'of', 'the']\n",
            "spirits\n",
            "......\n",
            "['spirits', 'the', 'the', 'computer']\n",
            "of\n",
            "......\n",
            "['of', 'spirits', 'computer', 'with']\n",
            "the\n",
            "......\n",
            "['the', 'of', 'with', 'our']\n",
            "computer\n",
            "......\n",
            "['computer', 'the', 'our', 'spells.']\n",
            "with\n",
            "......\n",
            "['are', 'we', 'to', 'study']\n",
            "about\n",
            "......\n",
            "['about', 'are', 'study', 'the']\n",
            "to\n",
            "......\n",
            "['to', 'about', 'the', 'idea']\n",
            "study\n",
            "......\n",
            "['study', 'to', 'idea', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'study', 'of', 'a']\n",
            "idea\n",
            "......\n",
            "['idea', 'the', 'a', 'computational']\n",
            "of\n",
            "......\n",
            "['of', 'idea', 'computational', 'process.']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'process.', 'computational']\n",
            "computational\n",
            "......\n",
            "['computational', 'a', 'computational', 'processes']\n",
            "process.\n",
            "......\n",
            "['process.', 'computational', 'processes', 'are']\n",
            "computational\n",
            "......\n",
            "['computational', 'process.', 'are', 'abstract']\n",
            "processes\n",
            "......\n",
            "['processes', 'computational', 'abstract', 'beings']\n",
            "are\n",
            "......\n",
            "['are', 'processes', 'beings', 'that']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'are', 'that', 'inhabit']\n",
            "beings\n",
            "......\n",
            "['beings', 'abstract', 'inhabit', 'computers.']\n",
            "that\n",
            "......\n",
            "['that', 'beings', 'computers.', 'as']\n",
            "inhabit\n",
            "......\n",
            "['inhabit', 'that', 'as', 'they']\n",
            "computers.\n",
            "......\n",
            "['computers.', 'inhabit', 'they', 'evolve,']\n",
            "as\n",
            "......\n",
            "['as', 'computers.', 'evolve,', 'processes']\n",
            "they\n",
            "......\n",
            "['they', 'as', 'processes', 'manipulate']\n",
            "evolve,\n",
            "......\n",
            "['evolve,', 'they', 'manipulate', 'other']\n",
            "processes\n",
            "......\n",
            "['processes', 'evolve,', 'other', 'abstract']\n",
            "manipulate\n",
            "......\n",
            "['manipulate', 'processes', 'abstract', 'things']\n",
            "other\n",
            "......\n",
            "['other', 'manipulate', 'things', 'called']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'other', 'called', 'data.']\n",
            "things\n",
            "......\n",
            "['things', 'abstract', 'data.', 'the']\n",
            "called\n",
            "......\n",
            "['called', 'things', 'the', 'evolution']\n",
            "data.\n",
            "......\n",
            "['data.', 'called', 'evolution', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'data.', 'of', 'a']\n",
            "evolution\n",
            "......\n",
            "['evolution', 'the', 'a', 'process']\n",
            "of\n",
            "......\n",
            "['of', 'evolution', 'process', 'is']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'is', 'directed']\n",
            "process\n",
            "......\n",
            "['process', 'a', 'directed', 'by']\n",
            "is\n",
            "......\n",
            "['is', 'process', 'by', 'a']\n",
            "directed\n",
            "......\n",
            "['directed', 'is', 'a', 'pattern']\n",
            "by\n",
            "......\n",
            "['by', 'directed', 'pattern', 'of']\n",
            "a\n",
            "......\n",
            "['a', 'by', 'of', 'rules']\n",
            "pattern\n",
            "......\n",
            "['pattern', 'a', 'rules', 'called']\n",
            "of\n",
            "......\n",
            "['of', 'pattern', 'called', 'a']\n",
            "rules\n",
            "......\n",
            "['rules', 'of', 'a', 'program.']\n",
            "called\n",
            "......\n",
            "['called', 'rules', 'program.', 'people']\n",
            "a\n",
            "......\n",
            "['a', 'called', 'people', 'create']\n",
            "program.\n",
            "......\n",
            "['program.', 'a', 'create', 'programs']\n",
            "people\n",
            "......\n",
            "['people', 'program.', 'programs', 'to']\n",
            "create\n",
            "......\n",
            "['create', 'people', 'to', 'direct']\n",
            "programs\n",
            "......\n",
            "['programs', 'create', 'direct', 'processes.']\n",
            "to\n",
            "......\n",
            "['to', 'programs', 'processes.', 'in']\n",
            "direct\n",
            "......\n",
            "['direct', 'to', 'in', 'effect,']\n",
            "processes.\n",
            "......\n",
            "['processes.', 'direct', 'effect,', 'we']\n",
            "in\n",
            "......\n",
            "['in', 'processes.', 'we', 'conjure']\n",
            "effect,\n",
            "......\n",
            "['effect,', 'in', 'conjure', 'the']\n",
            "we\n",
            "......\n",
            "['we', 'effect,', 'the', 'spirits']\n",
            "conjure\n",
            "......\n",
            "['conjure', 'we', 'spirits', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'conjure', 'of', 'the']\n",
            "spirits\n",
            "......\n",
            "['spirits', 'the', 'the', 'computer']\n",
            "of\n",
            "......\n",
            "['of', 'spirits', 'computer', 'with']\n",
            "the\n",
            "......\n",
            "['the', 'of', 'with', 'our']\n",
            "computer\n",
            "......\n",
            "['computer', 'the', 'our', 'spells.']\n",
            "with\n",
            "......\n",
            "['are', 'we', 'to', 'study']\n",
            "about\n",
            "......\n",
            "['about', 'are', 'study', 'the']\n",
            "to\n",
            "......\n",
            "['to', 'about', 'the', 'idea']\n",
            "study\n",
            "......\n",
            "['study', 'to', 'idea', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'study', 'of', 'a']\n",
            "idea\n",
            "......\n",
            "['idea', 'the', 'a', 'computational']\n",
            "of\n",
            "......\n",
            "['of', 'idea', 'computational', 'process.']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'process.', 'computational']\n",
            "computational\n",
            "......\n",
            "['computational', 'a', 'computational', 'processes']\n",
            "process.\n",
            "......\n",
            "['process.', 'computational', 'processes', 'are']\n",
            "computational\n",
            "......\n",
            "['computational', 'process.', 'are', 'abstract']\n",
            "processes\n",
            "......\n",
            "['processes', 'computational', 'abstract', 'beings']\n",
            "are\n",
            "......\n",
            "['are', 'processes', 'beings', 'that']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'are', 'that', 'inhabit']\n",
            "beings\n",
            "......\n",
            "['beings', 'abstract', 'inhabit', 'computers.']\n",
            "that\n",
            "......\n",
            "['that', 'beings', 'computers.', 'as']\n",
            "inhabit\n",
            "......\n",
            "['inhabit', 'that', 'as', 'they']\n",
            "computers.\n",
            "......\n",
            "['computers.', 'inhabit', 'they', 'evolve,']\n",
            "as\n",
            "......\n",
            "['as', 'computers.', 'evolve,', 'processes']\n",
            "they\n",
            "......\n",
            "['they', 'as', 'processes', 'manipulate']\n",
            "evolve,\n",
            "......\n",
            "['evolve,', 'they', 'manipulate', 'other']\n",
            "processes\n",
            "......\n",
            "['processes', 'evolve,', 'other', 'abstract']\n",
            "manipulate\n",
            "......\n",
            "['manipulate', 'processes', 'abstract', 'things']\n",
            "other\n",
            "......\n",
            "['other', 'manipulate', 'things', 'called']\n",
            "abstract\n",
            "......\n",
            "['abstract', 'other', 'called', 'data.']\n",
            "things\n",
            "......\n",
            "['things', 'abstract', 'data.', 'the']\n",
            "called\n",
            "......\n",
            "['called', 'things', 'the', 'evolution']\n",
            "data.\n",
            "......\n",
            "['data.', 'called', 'evolution', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'data.', 'of', 'a']\n",
            "evolution\n",
            "......\n",
            "['evolution', 'the', 'a', 'process']\n",
            "of\n",
            "......\n",
            "['of', 'evolution', 'process', 'is']\n",
            "a\n",
            "......\n",
            "['a', 'of', 'is', 'directed']\n",
            "process\n",
            "......\n",
            "['process', 'a', 'directed', 'by']\n",
            "is\n",
            "......\n",
            "['is', 'process', 'by', 'a']\n",
            "directed\n",
            "......\n",
            "['directed', 'is', 'a', 'pattern']\n",
            "by\n",
            "......\n",
            "['by', 'directed', 'pattern', 'of']\n",
            "a\n",
            "......\n",
            "['a', 'by', 'of', 'rules']\n",
            "pattern\n",
            "......\n",
            "['pattern', 'a', 'rules', 'called']\n",
            "of\n",
            "......\n",
            "['of', 'pattern', 'called', 'a']\n",
            "rules\n",
            "......\n",
            "['rules', 'of', 'a', 'program.']\n",
            "called\n",
            "......\n",
            "['called', 'rules', 'program.', 'people']\n",
            "a\n",
            "......\n",
            "['a', 'called', 'people', 'create']\n",
            "program.\n",
            "......\n",
            "['program.', 'a', 'create', 'programs']\n",
            "people\n",
            "......\n",
            "['people', 'program.', 'programs', 'to']\n",
            "create\n",
            "......\n",
            "['create', 'people', 'to', 'direct']\n",
            "programs\n",
            "......\n",
            "['programs', 'create', 'direct', 'processes.']\n",
            "to\n",
            "......\n",
            "['to', 'programs', 'processes.', 'in']\n",
            "direct\n",
            "......\n",
            "['direct', 'to', 'in', 'effect,']\n",
            "processes.\n",
            "......\n",
            "['processes.', 'direct', 'effect,', 'we']\n",
            "in\n",
            "......\n",
            "['in', 'processes.', 'we', 'conjure']\n",
            "effect,\n",
            "......\n",
            "['effect,', 'in', 'conjure', 'the']\n",
            "we\n",
            "......\n",
            "['we', 'effect,', 'the', 'spirits']\n",
            "conjure\n",
            "......\n",
            "['conjure', 'we', 'spirits', 'of']\n",
            "the\n",
            "......\n",
            "['the', 'conjure', 'of', 'the']\n",
            "spirits\n",
            "......\n",
            "['spirits', 'the', 'the', 'computer']\n",
            "of\n",
            "......\n",
            "['of', 'spirits', 'computer', 'with']\n",
            "the\n",
            "......\n",
            "['the', 'of', 'with', 'our']\n",
            "computer\n",
            "......\n",
            "['computer', 'the', 'our', 'spells.']\n",
            "with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "cQVLze5Xp4oF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Matriz de embeddings\n",
        "embedding_dim = 10\n",
        "\n",
        "np.random.random((vocab_size, embedding_dim)).astype(np.float32)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "print(embeddings)\n",
        "\n",
        "E = torch.tensor(np.random.random((vocab_size, embedding_dim)).astype(np.float32))\n",
        "print(E.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "43-3KLO5pgwu",
        "outputId": "3082d63a-0f3f-4c2c-d193-2b7153d857f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(46, 10)\n",
            "torch.Size([46, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Asociar palabra a fila que quieres\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "data = []\n",
        "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
        "    context = (\n",
        "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
        "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
        "    )\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "print(data[:5])\n",
        "\n",
        "word_to_ix['with']"
      ],
      "metadata": {
        "id": "4yXtoLGyrd7F",
        "outputId": "aa642d91-67fb-4cb6-8851-3e4654688a0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['are', 'we', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study'), (['study', 'to', 'idea', 'of'], 'the'), (['the', 'study', 'of', 'a'], 'idea')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "E[word_to_ix['with']].shape"
      ],
      "metadata": {
        "id": "tqTWYEQLsoPI",
        "outputId": "0dc8b45c-30d4-4783-d889-36c020763db8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters=1\n",
        "\n",
        "for e in range (max_iters):\n",
        "  for x, t in data:\n",
        "    e_list=[]\n",
        "    for p in x:\n",
        "      ix=[p]\n",
        "      ix.append(word_to_ix[p])\n",
        "      e_list.append(E[word_to_ix[p],:])\n",
        "    print(ix)\n",
        "    print(e_list)\n",
        "    print(t)\n",
        "\n",
        "    print(\"......\")"
      ],
      "metadata": {
        "id": "CnKf6de-uULH",
        "outputId": "08d3c30a-7700-4955-bab4-1dfda085cb95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['study', 40]\n",
            "[tensor([0.2954, 0.8656, 0.3396, 0.5681, 0.7535, 0.4349, 0.4316, 0.4111, 0.0571,\n",
            "        0.5633]), tensor([0.0255, 0.9877, 0.1933, 0.9089, 0.9616, 0.1103, 0.7102, 0.6352, 0.8589,\n",
            "        0.9440]), tensor([0.7541, 0.6077, 0.2650, 0.5333, 0.9792, 0.6850, 0.0662, 0.9781, 0.2187,\n",
            "        0.0377]), tensor([0.2922, 0.9507, 0.8412, 0.7651, 0.3008, 0.6587, 0.9570, 0.1500, 0.1978,\n",
            "        0.5724])]\n",
            "about\n",
            "......\n",
            "['the', 38]\n",
            "[tensor([3.5641e-01, 6.5804e-01, 8.7291e-01, 1.9301e-01, 7.1936e-01, 7.5361e-04,\n",
            "        7.1932e-01, 8.6467e-02, 9.5575e-01, 2.9149e-02]), tensor([0.2954, 0.8656, 0.3396, 0.5681, 0.7535, 0.4349, 0.4316, 0.4111, 0.0571,\n",
            "        0.5633]), tensor([0.2922, 0.9507, 0.8412, 0.7651, 0.3008, 0.6587, 0.9570, 0.1500, 0.1978,\n",
            "        0.5724]), tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345])]\n",
            "to\n",
            "......\n",
            "['idea', 4]\n",
            "[tensor([0.7541, 0.6077, 0.2650, 0.5333, 0.9792, 0.6850, 0.0662, 0.9781, 0.2187,\n",
            "        0.0377]), tensor([3.5641e-01, 6.5804e-01, 8.7291e-01, 1.9301e-01, 7.1936e-01, 7.5361e-04,\n",
            "        7.1932e-01, 8.6467e-02, 9.5575e-01, 2.9149e-02]), tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345]), tensor([0.0025, 0.3421, 0.8336, 0.9975, 0.9476, 0.7174, 0.9809, 0.3669, 0.4363,\n",
            "        0.7169])]\n",
            "study\n",
            "......\n",
            "['of', 0]\n",
            "[tensor([0.2922, 0.9507, 0.8412, 0.7651, 0.3008, 0.6587, 0.9570, 0.1500, 0.1978,\n",
            "        0.5724]), tensor([0.7541, 0.6077, 0.2650, 0.5333, 0.9792, 0.6850, 0.0662, 0.9781, 0.2187,\n",
            "        0.0377]), tensor([0.0025, 0.3421, 0.8336, 0.9975, 0.9476, 0.7174, 0.9809, 0.3669, 0.4363,\n",
            "        0.7169]), tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106])]\n",
            "the\n",
            "......\n",
            "['a', 25]\n",
            "[tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345]), tensor([0.2922, 0.9507, 0.8412, 0.7651, 0.3008, 0.6587, 0.9570, 0.1500, 0.1978,\n",
            "        0.5724]), tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106]), tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01])]\n",
            "idea\n",
            "......\n",
            "['computational', 19]\n",
            "[tensor([0.0025, 0.3421, 0.8336, 0.9975, 0.9476, 0.7174, 0.9809, 0.3669, 0.4363,\n",
            "        0.7169]), tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345]), tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01]), tensor([0.8555, 0.1850, 0.0628, 0.5365, 0.9692, 0.6533, 0.1400, 0.0446, 0.7462,\n",
            "        0.6707])]\n",
            "of\n",
            "......\n",
            "['process.', 41]\n",
            "[tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106]), tensor([0.0025, 0.3421, 0.8336, 0.9975, 0.9476, 0.7174, 0.9809, 0.3669, 0.4363,\n",
            "        0.7169]), tensor([0.8555, 0.1850, 0.0628, 0.5365, 0.9692, 0.6533, 0.1400, 0.0446, 0.7462,\n",
            "        0.6707]), tensor([0.8396, 0.1230, 0.8387, 0.3730, 0.4745, 0.9693, 0.4891, 0.5170, 0.1835,\n",
            "        0.7326])]\n",
            "a\n",
            "......\n",
            "['computational', 19]\n",
            "[tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01]), tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106]), tensor([0.8396, 0.1230, 0.8387, 0.3730, 0.4745, 0.9693, 0.4891, 0.5170, 0.1835,\n",
            "        0.7326]), tensor([0.8555, 0.1850, 0.0628, 0.5365, 0.9692, 0.6533, 0.1400, 0.0446, 0.7462,\n",
            "        0.6707])]\n",
            "computational\n",
            "......\n",
            "['processes', 6]\n",
            "[tensor([0.8555, 0.1850, 0.0628, 0.5365, 0.9692, 0.6533, 0.1400, 0.0446, 0.7462,\n",
            "        0.6707]), tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01]), tensor([0.8555, 0.1850, 0.0628, 0.5365, 0.9692, 0.6533, 0.1400, 0.0446, 0.7462,\n",
            "        0.6707]), tensor([0.0389, 0.8203, 0.2375, 0.7831, 0.8995, 0.1431, 0.0568, 0.8820, 0.1553,\n",
            "        0.9168])]\n",
            "process.\n",
            "......\n",
            "['are', 2]\n",
            "[tensor([0.8396, 0.1230, 0.8387, 0.3730, 0.4745, 0.9693, 0.4891, 0.5170, 0.1835,\n",
            "        0.7326]), tensor([0.8555, 0.1850, 0.0628, 0.5365, 0.9692, 0.6533, 0.1400, 0.0446, 0.7462,\n",
            "        0.6707]), tensor([0.0389, 0.8203, 0.2375, 0.7831, 0.8995, 0.1431, 0.0568, 0.8820, 0.1553,\n",
            "        0.9168]), tensor([0.2954, 0.8656, 0.3396, 0.5681, 0.7535, 0.4349, 0.4316, 0.4111, 0.0571,\n",
            "        0.5633])]\n",
            "computational\n",
            "......\n",
            "['abstract', 37]\n",
            "[tensor([0.8555, 0.1850, 0.0628, 0.5365, 0.9692, 0.6533, 0.1400, 0.0446, 0.7462,\n",
            "        0.6707]), tensor([0.8396, 0.1230, 0.8387, 0.3730, 0.4745, 0.9693, 0.4891, 0.5170, 0.1835,\n",
            "        0.7326]), tensor([0.2954, 0.8656, 0.3396, 0.5681, 0.7535, 0.4349, 0.4316, 0.4111, 0.0571,\n",
            "        0.5633]), tensor([0.0790, 0.4167, 0.0770, 0.8071, 0.5014, 0.5781, 0.9108, 0.8007, 0.6143,\n",
            "        0.3705])]\n",
            "processes\n",
            "......\n",
            "['beings', 7]\n",
            "[tensor([0.0389, 0.8203, 0.2375, 0.7831, 0.8995, 0.1431, 0.0568, 0.8820, 0.1553,\n",
            "        0.9168]), tensor([0.8555, 0.1850, 0.0628, 0.5365, 0.9692, 0.6533, 0.1400, 0.0446, 0.7462,\n",
            "        0.6707]), tensor([0.0790, 0.4167, 0.0770, 0.8071, 0.5014, 0.5781, 0.9108, 0.8007, 0.6143,\n",
            "        0.3705]), tensor([0.0834, 0.2763, 0.0643, 0.9000, 0.5151, 0.6122, 0.8516, 0.6763, 0.4202,\n",
            "        0.0273])]\n",
            "are\n",
            "......\n",
            "['that', 17]\n",
            "[tensor([0.2954, 0.8656, 0.3396, 0.5681, 0.7535, 0.4349, 0.4316, 0.4111, 0.0571,\n",
            "        0.5633]), tensor([0.0389, 0.8203, 0.2375, 0.7831, 0.8995, 0.1431, 0.0568, 0.8820, 0.1553,\n",
            "        0.9168]), tensor([0.0834, 0.2763, 0.0643, 0.9000, 0.5151, 0.6122, 0.8516, 0.6763, 0.4202,\n",
            "        0.0273]), tensor([0.5515, 0.4954, 0.7704, 0.9237, 0.7898, 0.3182, 0.0628, 0.7699, 0.5600,\n",
            "        0.2589])]\n",
            "abstract\n",
            "......\n",
            "['inhabit', 24]\n",
            "[tensor([0.0790, 0.4167, 0.0770, 0.8071, 0.5014, 0.5781, 0.9108, 0.8007, 0.6143,\n",
            "        0.3705]), tensor([0.2954, 0.8656, 0.3396, 0.5681, 0.7535, 0.4349, 0.4316, 0.4111, 0.0571,\n",
            "        0.5633]), tensor([0.5515, 0.4954, 0.7704, 0.9237, 0.7898, 0.3182, 0.0628, 0.7699, 0.5600,\n",
            "        0.2589]), tensor([0.0459, 0.3382, 0.2305, 0.1065, 0.4758, 0.2325, 0.5829, 0.0525, 0.5794,\n",
            "        0.6673])]\n",
            "beings\n",
            "......\n",
            "['computers.', 29]\n",
            "[tensor([0.0834, 0.2763, 0.0643, 0.9000, 0.5151, 0.6122, 0.8516, 0.6763, 0.4202,\n",
            "        0.0273]), tensor([0.0790, 0.4167, 0.0770, 0.8071, 0.5014, 0.5781, 0.9108, 0.8007, 0.6143,\n",
            "        0.3705]), tensor([0.0459, 0.3382, 0.2305, 0.1065, 0.4758, 0.2325, 0.5829, 0.0525, 0.5794,\n",
            "        0.6673]), tensor([0.8407, 0.9290, 0.0483, 0.7696, 0.8113, 0.5218, 0.0055, 0.8362, 0.5157,\n",
            "        0.1860])]\n",
            "that\n",
            "......\n",
            "['as', 30]\n",
            "[tensor([0.5515, 0.4954, 0.7704, 0.9237, 0.7898, 0.3182, 0.0628, 0.7699, 0.5600,\n",
            "        0.2589]), tensor([0.0834, 0.2763, 0.0643, 0.9000, 0.5151, 0.6122, 0.8516, 0.6763, 0.4202,\n",
            "        0.0273]), tensor([0.8407, 0.9290, 0.0483, 0.7696, 0.8113, 0.5218, 0.0055, 0.8362, 0.5157,\n",
            "        0.1860]), tensor([0.1979, 0.6726, 0.6790, 0.5826, 0.7941, 0.2763, 0.3309, 0.2226, 0.9822,\n",
            "        0.4672])]\n",
            "inhabit\n",
            "......\n",
            "['they', 35]\n",
            "[tensor([0.0459, 0.3382, 0.2305, 0.1065, 0.4758, 0.2325, 0.5829, 0.0525, 0.5794,\n",
            "        0.6673]), tensor([0.5515, 0.4954, 0.7704, 0.9237, 0.7898, 0.3182, 0.0628, 0.7699, 0.5600,\n",
            "        0.2589]), tensor([0.1979, 0.6726, 0.6790, 0.5826, 0.7941, 0.2763, 0.3309, 0.2226, 0.9822,\n",
            "        0.4672]), tensor([0.7803, 0.5217, 0.2617, 0.5502, 0.0209, 0.0619, 0.0667, 0.5418, 0.0083,\n",
            "        0.1905])]\n",
            "computers.\n",
            "......\n",
            "['evolve,', 12]\n",
            "[tensor([0.8407, 0.9290, 0.0483, 0.7696, 0.8113, 0.5218, 0.0055, 0.8362, 0.5157,\n",
            "        0.1860]), tensor([0.0459, 0.3382, 0.2305, 0.1065, 0.4758, 0.2325, 0.5829, 0.0525, 0.5794,\n",
            "        0.6673]), tensor([0.7803, 0.5217, 0.2617, 0.5502, 0.0209, 0.0619, 0.0667, 0.5418, 0.0083,\n",
            "        0.1905]), tensor([0.5511, 0.6649, 0.3028, 0.9470, 0.3417, 0.5312, 0.0317, 0.1458, 0.5345,\n",
            "        0.7886])]\n",
            "as\n",
            "......\n",
            "['processes', 6]\n",
            "[tensor([0.1979, 0.6726, 0.6790, 0.5826, 0.7941, 0.2763, 0.3309, 0.2226, 0.9822,\n",
            "        0.4672]), tensor([0.8407, 0.9290, 0.0483, 0.7696, 0.8113, 0.5218, 0.0055, 0.8362, 0.5157,\n",
            "        0.1860]), tensor([0.5511, 0.6649, 0.3028, 0.9470, 0.3417, 0.5312, 0.0317, 0.1458, 0.5345,\n",
            "        0.7886]), tensor([0.0389, 0.8203, 0.2375, 0.7831, 0.8995, 0.1431, 0.0568, 0.8820, 0.1553,\n",
            "        0.9168])]\n",
            "they\n",
            "......\n",
            "['manipulate', 8]\n",
            "[tensor([0.7803, 0.5217, 0.2617, 0.5502, 0.0209, 0.0619, 0.0667, 0.5418, 0.0083,\n",
            "        0.1905]), tensor([0.1979, 0.6726, 0.6790, 0.5826, 0.7941, 0.2763, 0.3309, 0.2226, 0.9822,\n",
            "        0.4672]), tensor([0.0389, 0.8203, 0.2375, 0.7831, 0.8995, 0.1431, 0.0568, 0.8820, 0.1553,\n",
            "        0.9168]), tensor([0.3116, 0.0817, 0.5868, 0.7414, 0.5828, 0.3579, 0.6462, 0.5431, 0.2973,\n",
            "        0.7003])]\n",
            "evolve,\n",
            "......\n",
            "['other', 33]\n",
            "[tensor([0.5511, 0.6649, 0.3028, 0.9470, 0.3417, 0.5312, 0.0317, 0.1458, 0.5345,\n",
            "        0.7886]), tensor([0.7803, 0.5217, 0.2617, 0.5502, 0.0209, 0.0619, 0.0667, 0.5418, 0.0083,\n",
            "        0.1905]), tensor([0.3116, 0.0817, 0.5868, 0.7414, 0.5828, 0.3579, 0.6462, 0.5431, 0.2973,\n",
            "        0.7003]), tensor([0.0962, 0.5133, 0.7516, 0.2514, 0.2529, 0.2841, 0.8373, 0.9580, 0.5612,\n",
            "        0.3900])]\n",
            "processes\n",
            "......\n",
            "['abstract', 37]\n",
            "[tensor([0.0389, 0.8203, 0.2375, 0.7831, 0.8995, 0.1431, 0.0568, 0.8820, 0.1553,\n",
            "        0.9168]), tensor([0.5511, 0.6649, 0.3028, 0.9470, 0.3417, 0.5312, 0.0317, 0.1458, 0.5345,\n",
            "        0.7886]), tensor([0.0962, 0.5133, 0.7516, 0.2514, 0.2529, 0.2841, 0.8373, 0.9580, 0.5612,\n",
            "        0.3900]), tensor([0.0790, 0.4167, 0.0770, 0.8071, 0.5014, 0.5781, 0.9108, 0.8007, 0.6143,\n",
            "        0.3705])]\n",
            "manipulate\n",
            "......\n",
            "['things', 27]\n",
            "[tensor([0.3116, 0.0817, 0.5868, 0.7414, 0.5828, 0.3579, 0.6462, 0.5431, 0.2973,\n",
            "        0.7003]), tensor([0.0389, 0.8203, 0.2375, 0.7831, 0.8995, 0.1431, 0.0568, 0.8820, 0.1553,\n",
            "        0.9168]), tensor([0.0790, 0.4167, 0.0770, 0.8071, 0.5014, 0.5781, 0.9108, 0.8007, 0.6143,\n",
            "        0.3705]), tensor([0.2016, 0.1060, 0.8849, 0.0925, 0.5417, 0.1257, 0.9347, 0.5680, 0.3632,\n",
            "        0.6013])]\n",
            "other\n",
            "......\n",
            "['called', 44]\n",
            "[tensor([0.0962, 0.5133, 0.7516, 0.2514, 0.2529, 0.2841, 0.8373, 0.9580, 0.5612,\n",
            "        0.3900]), tensor([0.3116, 0.0817, 0.5868, 0.7414, 0.5828, 0.3579, 0.6462, 0.5431, 0.2973,\n",
            "        0.7003]), tensor([0.2016, 0.1060, 0.8849, 0.0925, 0.5417, 0.1257, 0.9347, 0.5680, 0.3632,\n",
            "        0.6013]), tensor([0.4573, 0.9308, 0.4797, 0.4600, 0.3952, 0.4801, 0.2736, 0.9909, 0.7454,\n",
            "        0.3784])]\n",
            "abstract\n",
            "......\n",
            "['data.', 36]\n",
            "[tensor([0.0790, 0.4167, 0.0770, 0.8071, 0.5014, 0.5781, 0.9108, 0.8007, 0.6143,\n",
            "        0.3705]), tensor([0.0962, 0.5133, 0.7516, 0.2514, 0.2529, 0.2841, 0.8373, 0.9580, 0.5612,\n",
            "        0.3900]), tensor([0.4573, 0.9308, 0.4797, 0.4600, 0.3952, 0.4801, 0.2736, 0.9909, 0.7454,\n",
            "        0.3784]), tensor([0.4374, 0.4591, 0.2042, 0.0662, 0.3938, 0.3545, 0.4595, 0.0581, 0.6911,\n",
            "        0.6618])]\n",
            "things\n",
            "......\n",
            "['the', 38]\n",
            "[tensor([0.2016, 0.1060, 0.8849, 0.0925, 0.5417, 0.1257, 0.9347, 0.5680, 0.3632,\n",
            "        0.6013]), tensor([0.0790, 0.4167, 0.0770, 0.8071, 0.5014, 0.5781, 0.9108, 0.8007, 0.6143,\n",
            "        0.3705]), tensor([0.4374, 0.4591, 0.2042, 0.0662, 0.3938, 0.3545, 0.4595, 0.0581, 0.6911,\n",
            "        0.6618]), tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345])]\n",
            "called\n",
            "......\n",
            "['evolution', 10]\n",
            "[tensor([0.4573, 0.9308, 0.4797, 0.4600, 0.3952, 0.4801, 0.2736, 0.9909, 0.7454,\n",
            "        0.3784]), tensor([0.2016, 0.1060, 0.8849, 0.0925, 0.5417, 0.1257, 0.9347, 0.5680, 0.3632,\n",
            "        0.6013]), tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345]), tensor([0.5533, 0.1421, 0.7381, 0.0740, 0.7174, 0.6604, 0.8472, 0.3754, 0.7938,\n",
            "        0.4815])]\n",
            "data.\n",
            "......\n",
            "['of', 0]\n",
            "[tensor([0.4374, 0.4591, 0.2042, 0.0662, 0.3938, 0.3545, 0.4595, 0.0581, 0.6911,\n",
            "        0.6618]), tensor([0.4573, 0.9308, 0.4797, 0.4600, 0.3952, 0.4801, 0.2736, 0.9909, 0.7454,\n",
            "        0.3784]), tensor([0.5533, 0.1421, 0.7381, 0.0740, 0.7174, 0.6604, 0.8472, 0.3754, 0.7938,\n",
            "        0.4815]), tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106])]\n",
            "the\n",
            "......\n",
            "['a', 25]\n",
            "[tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345]), tensor([0.4374, 0.4591, 0.2042, 0.0662, 0.3938, 0.3545, 0.4595, 0.0581, 0.6911,\n",
            "        0.6618]), tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106]), tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01])]\n",
            "evolution\n",
            "......\n",
            "['process', 31]\n",
            "[tensor([0.5533, 0.1421, 0.7381, 0.0740, 0.7174, 0.6604, 0.8472, 0.3754, 0.7938,\n",
            "        0.4815]), tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345]), tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01]), tensor([0.8333, 0.6704, 0.0233, 0.9602, 0.0990, 0.5869, 0.3888, 0.4009, 0.2833,\n",
            "        0.4508])]\n",
            "of\n",
            "......\n",
            "['is', 26]\n",
            "[tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106]), tensor([0.5533, 0.1421, 0.7381, 0.0740, 0.7174, 0.6604, 0.8472, 0.3754, 0.7938,\n",
            "        0.4815]), tensor([0.8333, 0.6704, 0.0233, 0.9602, 0.0990, 0.5869, 0.3888, 0.4009, 0.2833,\n",
            "        0.4508]), tensor([0.9756, 0.3227, 0.1388, 0.1918, 0.5284, 0.4769, 0.5640, 0.5189, 0.7927,\n",
            "        0.1453])]\n",
            "a\n",
            "......\n",
            "['directed', 42]\n",
            "[tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01]), tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106]), tensor([0.9756, 0.3227, 0.1388, 0.1918, 0.5284, 0.4769, 0.5640, 0.5189, 0.7927,\n",
            "        0.1453]), tensor([0.4525, 0.6177, 0.8622, 0.3794, 0.6039, 0.5587, 0.4942, 0.9656, 0.5489,\n",
            "        0.7896])]\n",
            "process\n",
            "......\n",
            "['by', 14]\n",
            "[tensor([0.8333, 0.6704, 0.0233, 0.9602, 0.0990, 0.5869, 0.3888, 0.4009, 0.2833,\n",
            "        0.4508]), tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01]), tensor([0.4525, 0.6177, 0.8622, 0.3794, 0.6039, 0.5587, 0.4942, 0.9656, 0.5489,\n",
            "        0.7896]), tensor([0.5486, 0.0376, 0.9199, 0.1178, 0.7782, 0.9242, 0.9219, 0.5660, 0.0960,\n",
            "        0.6927])]\n",
            "is\n",
            "......\n",
            "['a', 25]\n",
            "[tensor([0.9756, 0.3227, 0.1388, 0.1918, 0.5284, 0.4769, 0.5640, 0.5189, 0.7927,\n",
            "        0.1453]), tensor([0.8333, 0.6704, 0.0233, 0.9602, 0.0990, 0.5869, 0.3888, 0.4009, 0.2833,\n",
            "        0.4508]), tensor([0.5486, 0.0376, 0.9199, 0.1178, 0.7782, 0.9242, 0.9219, 0.5660, 0.0960,\n",
            "        0.6927]), tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01])]\n",
            "directed\n",
            "......\n",
            "['pattern', 39]\n",
            "[tensor([0.4525, 0.6177, 0.8622, 0.3794, 0.6039, 0.5587, 0.4942, 0.9656, 0.5489,\n",
            "        0.7896]), tensor([0.9756, 0.3227, 0.1388, 0.1918, 0.5284, 0.4769, 0.5640, 0.5189, 0.7927,\n",
            "        0.1453]), tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01]), tensor([0.0292, 0.5993, 0.7358, 0.4298, 0.3468, 0.9365, 0.5048, 0.6178, 0.3269,\n",
            "        0.2722])]\n",
            "by\n",
            "......\n",
            "['of', 0]\n",
            "[tensor([0.5486, 0.0376, 0.9199, 0.1178, 0.7782, 0.9242, 0.9219, 0.5660, 0.0960,\n",
            "        0.6927]), tensor([0.4525, 0.6177, 0.8622, 0.3794, 0.6039, 0.5587, 0.4942, 0.9656, 0.5489,\n",
            "        0.7896]), tensor([0.0292, 0.5993, 0.7358, 0.4298, 0.3468, 0.9365, 0.5048, 0.6178, 0.3269,\n",
            "        0.2722]), tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106])]\n",
            "a\n",
            "......\n",
            "['rules', 5]\n",
            "[tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01]), tensor([0.5486, 0.0376, 0.9199, 0.1178, 0.7782, 0.9242, 0.9219, 0.5660, 0.0960,\n",
            "        0.6927]), tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106]), tensor([0.0669, 0.8067, 0.8330, 0.3180, 0.1829, 0.3004, 0.4295, 0.3840, 0.5069,\n",
            "        0.4538])]\n",
            "pattern\n",
            "......\n",
            "['called', 44]\n",
            "[tensor([0.0292, 0.5993, 0.7358, 0.4298, 0.3468, 0.9365, 0.5048, 0.6178, 0.3269,\n",
            "        0.2722]), tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01]), tensor([0.0669, 0.8067, 0.8330, 0.3180, 0.1829, 0.3004, 0.4295, 0.3840, 0.5069,\n",
            "        0.4538]), tensor([0.4573, 0.9308, 0.4797, 0.4600, 0.3952, 0.4801, 0.2736, 0.9909, 0.7454,\n",
            "        0.3784])]\n",
            "of\n",
            "......\n",
            "['a', 25]\n",
            "[tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106]), tensor([0.0292, 0.5993, 0.7358, 0.4298, 0.3468, 0.9365, 0.5048, 0.6178, 0.3269,\n",
            "        0.2722]), tensor([0.4573, 0.9308, 0.4797, 0.4600, 0.3952, 0.4801, 0.2736, 0.9909, 0.7454,\n",
            "        0.3784]), tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01])]\n",
            "rules\n",
            "......\n",
            "['program.', 18]\n",
            "[tensor([0.0669, 0.8067, 0.8330, 0.3180, 0.1829, 0.3004, 0.4295, 0.3840, 0.5069,\n",
            "        0.4538]), tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106]), tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01]), tensor([0.2441, 0.3722, 0.5714, 0.7930, 0.5031, 0.4617, 0.5886, 0.7209, 0.1434,\n",
            "        0.6232])]\n",
            "called\n",
            "......\n",
            "['people', 20]\n",
            "[tensor([0.4573, 0.9308, 0.4797, 0.4600, 0.3952, 0.4801, 0.2736, 0.9909, 0.7454,\n",
            "        0.3784]), tensor([0.0669, 0.8067, 0.8330, 0.3180, 0.1829, 0.3004, 0.4295, 0.3840, 0.5069,\n",
            "        0.4538]), tensor([0.2441, 0.3722, 0.5714, 0.7930, 0.5031, 0.4617, 0.5886, 0.7209, 0.1434,\n",
            "        0.6232]), tensor([0.3866, 0.9301, 0.0510, 0.2998, 0.9282, 0.2886, 0.5576, 0.7059, 0.9005,\n",
            "        0.1800])]\n",
            "a\n",
            "......\n",
            "['create', 22]\n",
            "[tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01]), tensor([0.4573, 0.9308, 0.4797, 0.4600, 0.3952, 0.4801, 0.2736, 0.9909, 0.7454,\n",
            "        0.3784]), tensor([0.3866, 0.9301, 0.0510, 0.2998, 0.9282, 0.2886, 0.5576, 0.7059, 0.9005,\n",
            "        0.1800]), tensor([0.5396, 0.1899, 0.8197, 0.0823, 0.7198, 0.7800, 0.1422, 0.8456, 0.4740,\n",
            "        0.2569])]\n",
            "program.\n",
            "......\n",
            "['programs', 21]\n",
            "[tensor([0.2441, 0.3722, 0.5714, 0.7930, 0.5031, 0.4617, 0.5886, 0.7209, 0.1434,\n",
            "        0.6232]), tensor([1.0897e-04, 8.4669e-01, 5.8756e-01, 6.6702e-01, 6.8547e-01, 1.7729e-02,\n",
            "        6.7131e-01, 4.6275e-01, 7.9817e-01, 5.1130e-01]), tensor([0.5396, 0.1899, 0.8197, 0.0823, 0.7198, 0.7800, 0.1422, 0.8456, 0.4740,\n",
            "        0.2569]), tensor([0.2597, 0.4705, 0.4816, 0.9406, 0.0091, 0.5057, 0.0227, 0.6598, 0.1784,\n",
            "        0.7095])]\n",
            "people\n",
            "......\n",
            "['to', 15]\n",
            "[tensor([0.3866, 0.9301, 0.0510, 0.2998, 0.9282, 0.2886, 0.5576, 0.7059, 0.9005,\n",
            "        0.1800]), tensor([0.2441, 0.3722, 0.5714, 0.7930, 0.5031, 0.4617, 0.5886, 0.7209, 0.1434,\n",
            "        0.6232]), tensor([0.2597, 0.4705, 0.4816, 0.9406, 0.0091, 0.5057, 0.0227, 0.6598, 0.1784,\n",
            "        0.7095]), tensor([0.7541, 0.6077, 0.2650, 0.5333, 0.9792, 0.6850, 0.0662, 0.9781, 0.2187,\n",
            "        0.0377])]\n",
            "create\n",
            "......\n",
            "['direct', 9]\n",
            "[tensor([0.5396, 0.1899, 0.8197, 0.0823, 0.7198, 0.7800, 0.1422, 0.8456, 0.4740,\n",
            "        0.2569]), tensor([0.3866, 0.9301, 0.0510, 0.2998, 0.9282, 0.2886, 0.5576, 0.7059, 0.9005,\n",
            "        0.1800]), tensor([0.7541, 0.6077, 0.2650, 0.5333, 0.9792, 0.6850, 0.0662, 0.9781, 0.2187,\n",
            "        0.0377]), tensor([0.8897, 0.4188, 0.9596, 0.9991, 0.1304, 0.2579, 0.5253, 0.9483, 0.2880,\n",
            "        0.6620])]\n",
            "programs\n",
            "......\n",
            "['processes.', 28]\n",
            "[tensor([0.2597, 0.4705, 0.4816, 0.9406, 0.0091, 0.5057, 0.0227, 0.6598, 0.1784,\n",
            "        0.7095]), tensor([0.5396, 0.1899, 0.8197, 0.0823, 0.7198, 0.7800, 0.1422, 0.8456, 0.4740,\n",
            "        0.2569]), tensor([0.8897, 0.4188, 0.9596, 0.9991, 0.1304, 0.2579, 0.5253, 0.9483, 0.2880,\n",
            "        0.6620]), tensor([0.1679, 0.7443, 0.0552, 0.6954, 0.0760, 0.1097, 0.8877, 0.1818, 0.7233,\n",
            "        0.3826])]\n",
            "to\n",
            "......\n",
            "['in', 1]\n",
            "[tensor([0.7541, 0.6077, 0.2650, 0.5333, 0.9792, 0.6850, 0.0662, 0.9781, 0.2187,\n",
            "        0.0377]), tensor([0.2597, 0.4705, 0.4816, 0.9406, 0.0091, 0.5057, 0.0227, 0.6598, 0.1784,\n",
            "        0.7095]), tensor([0.1679, 0.7443, 0.0552, 0.6954, 0.0760, 0.1097, 0.8877, 0.1818, 0.7233,\n",
            "        0.3826]), tensor([0.7073, 0.9739, 0.0405, 0.4847, 0.9479, 0.1371, 0.3009, 0.5543, 0.6869,\n",
            "        0.1286])]\n",
            "direct\n",
            "......\n",
            "['effect,', 11]\n",
            "[tensor([0.8897, 0.4188, 0.9596, 0.9991, 0.1304, 0.2579, 0.5253, 0.9483, 0.2880,\n",
            "        0.6620]), tensor([0.7541, 0.6077, 0.2650, 0.5333, 0.9792, 0.6850, 0.0662, 0.9781, 0.2187,\n",
            "        0.0377]), tensor([0.7073, 0.9739, 0.0405, 0.4847, 0.9479, 0.1371, 0.3009, 0.5543, 0.6869,\n",
            "        0.1286]), tensor([0.4707, 0.9247, 0.0460, 0.7482, 0.6740, 0.1970, 0.6744, 0.1166, 0.3331,\n",
            "        0.7087])]\n",
            "processes.\n",
            "......\n",
            "['we', 3]\n",
            "[tensor([0.1679, 0.7443, 0.0552, 0.6954, 0.0760, 0.1097, 0.8877, 0.1818, 0.7233,\n",
            "        0.3826]), tensor([0.8897, 0.4188, 0.9596, 0.9991, 0.1304, 0.2579, 0.5253, 0.9483, 0.2880,\n",
            "        0.6620]), tensor([0.4707, 0.9247, 0.0460, 0.7482, 0.6740, 0.1970, 0.6744, 0.1166, 0.3331,\n",
            "        0.7087]), tensor([0.0255, 0.9877, 0.1933, 0.9089, 0.9616, 0.1103, 0.7102, 0.6352, 0.8589,\n",
            "        0.9440])]\n",
            "in\n",
            "......\n",
            "['conjure', 13]\n",
            "[tensor([0.7073, 0.9739, 0.0405, 0.4847, 0.9479, 0.1371, 0.3009, 0.5543, 0.6869,\n",
            "        0.1286]), tensor([0.1679, 0.7443, 0.0552, 0.6954, 0.0760, 0.1097, 0.8877, 0.1818, 0.7233,\n",
            "        0.3826]), tensor([0.0255, 0.9877, 0.1933, 0.9089, 0.9616, 0.1103, 0.7102, 0.6352, 0.8589,\n",
            "        0.9440]), tensor([0.6995, 0.6356, 0.7780, 0.5149, 0.1333, 0.2591, 0.7827, 0.9967, 0.9424,\n",
            "        0.0849])]\n",
            "effect,\n",
            "......\n",
            "['the', 38]\n",
            "[tensor([0.4707, 0.9247, 0.0460, 0.7482, 0.6740, 0.1970, 0.6744, 0.1166, 0.3331,\n",
            "        0.7087]), tensor([0.7073, 0.9739, 0.0405, 0.4847, 0.9479, 0.1371, 0.3009, 0.5543, 0.6869,\n",
            "        0.1286]), tensor([0.6995, 0.6356, 0.7780, 0.5149, 0.1333, 0.2591, 0.7827, 0.9967, 0.9424,\n",
            "        0.0849]), tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345])]\n",
            "we\n",
            "......\n",
            "['spirits', 43]\n",
            "[tensor([0.0255, 0.9877, 0.1933, 0.9089, 0.9616, 0.1103, 0.7102, 0.6352, 0.8589,\n",
            "        0.9440]), tensor([0.4707, 0.9247, 0.0460, 0.7482, 0.6740, 0.1970, 0.6744, 0.1166, 0.3331,\n",
            "        0.7087]), tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345]), tensor([0.3023, 0.6670, 0.3785, 0.0696, 0.0664, 0.0013, 0.3427, 0.7982, 0.4284,\n",
            "        0.8470])]\n",
            "conjure\n",
            "......\n",
            "['of', 0]\n",
            "[tensor([0.6995, 0.6356, 0.7780, 0.5149, 0.1333, 0.2591, 0.7827, 0.9967, 0.9424,\n",
            "        0.0849]), tensor([0.0255, 0.9877, 0.1933, 0.9089, 0.9616, 0.1103, 0.7102, 0.6352, 0.8589,\n",
            "        0.9440]), tensor([0.3023, 0.6670, 0.3785, 0.0696, 0.0664, 0.0013, 0.3427, 0.7982, 0.4284,\n",
            "        0.8470]), tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106])]\n",
            "the\n",
            "......\n",
            "['the', 38]\n",
            "[tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345]), tensor([0.6995, 0.6356, 0.7780, 0.5149, 0.1333, 0.2591, 0.7827, 0.9967, 0.9424,\n",
            "        0.0849]), tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106]), tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345])]\n",
            "spirits\n",
            "......\n",
            "['computer', 32]\n",
            "[tensor([0.3023, 0.6670, 0.3785, 0.0696, 0.0664, 0.0013, 0.3427, 0.7982, 0.4284,\n",
            "        0.8470]), tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345]), tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345]), tensor([0.4895, 0.8071, 0.3204, 0.9243, 0.4778, 0.0366, 0.7226, 0.3402, 0.7700,\n",
            "        0.3978])]\n",
            "of\n",
            "......\n",
            "['with', 34]\n",
            "[tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106]), tensor([0.3023, 0.6670, 0.3785, 0.0696, 0.0664, 0.0013, 0.3427, 0.7982, 0.4284,\n",
            "        0.8470]), tensor([0.4895, 0.8071, 0.3204, 0.9243, 0.4778, 0.0366, 0.7226, 0.3402, 0.7700,\n",
            "        0.3978]), tensor([0.1942, 0.7320, 0.2279, 0.4576, 0.1208, 0.5492, 0.3525, 0.8201, 0.9379,\n",
            "        0.3410])]\n",
            "the\n",
            "......\n",
            "['our', 45]\n",
            "[tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345]), tensor([0.3780, 0.4469, 0.4035, 0.2117, 0.1797, 0.7972, 0.7421, 0.2708, 0.2658,\n",
            "        0.4106]), tensor([0.1942, 0.7320, 0.2279, 0.4576, 0.1208, 0.5492, 0.3525, 0.8201, 0.9379,\n",
            "        0.3410]), tensor([0.0719, 0.3346, 0.6875, 0.6099, 0.4831, 0.3026, 0.3471, 0.8238, 0.8080,\n",
            "        0.6544])]\n",
            "computer\n",
            "......\n",
            "['spells.', 23]\n",
            "[tensor([0.4895, 0.8071, 0.3204, 0.9243, 0.4778, 0.0366, 0.7226, 0.3402, 0.7700,\n",
            "        0.3978]), tensor([0.7978, 0.2689, 0.6447, 0.3266, 0.1243, 0.6557, 0.3232, 0.1399, 0.9302,\n",
            "        0.7345]), tensor([0.0719, 0.3346, 0.6875, 0.6099, 0.4831, 0.3026, 0.3471, 0.8238, 0.8080,\n",
            "        0.6544]), tensor([0.8375, 0.8765, 0.3970, 0.3931, 0.9501, 0.4602, 0.6720, 0.0303, 0.8447,\n",
            "        0.7118])]\n",
            "with\n",
            "......\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NUEVO TEXTO\n",
        "\n",
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "raw_text = \"\"\"Ice skating is the self-propulsion and gliding of a person across an ice surface, using metal-bladed ice skates.\n",
        "People skate for various reasons, including recreation, exercise, competitive sports, and commuting.\n",
        "Ice skating may be performed on naturally frozen bodies of water, such as ponds, lakes, canals, and rivers,\n",
        "and on human-made ice surfaces both indoors and outdoors.\"\"\".split()\n",
        "\n",
        "# By deriving a set from `raw_text`, we deduplicate the array\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "data = []\n",
        "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
        "    context = (\n",
        "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
        "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
        "    )\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "print(data[:5])\n",
        "\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        pass\n",
        "\n",
        "# Create your model and train. Here are some functions to help you make\n",
        "# the data ready for use by your module.\n",
        "\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "make_context_vector(data[0][0], word_to_ix)  # example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_lqtfO6lg4b",
        "outputId": "2d73a70f-88d8-42a1-d812-4025326f4bda"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['skating', 'Ice', 'the', 'self-propulsion'], 'is'), (['is', 'skating', 'self-propulsion', 'and'], 'the'), (['the', 'is', 'and', 'gliding'], 'self-propulsion'), (['self-propulsion', 'the', 'gliding', 'of'], 'and'), (['and', 'self-propulsion', 'of', 'a'], 'gliding')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([26, 37, 39, 43])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}