{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29276b9-e840-40f5-afe8-cb644b27e0a1",
   "metadata": {},
   "source": [
    "## Competición del curso de redes neuronales\n",
    "\n",
    "**Elaborado por:** Jazmín y Jacqueline Fernández Ramírez. \n",
    "\n",
    "**Curso:** Redes neuronales y aprendizaje profundo.\n",
    "\n",
    "**CUNEF Universidad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307aaef-c361-45c2-81d3-26006dd13f17",
   "metadata": {},
   "source": [
    "### Importación de liberías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e261a8-4594-4e02-bd58-98650722eff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jazfe\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: '[WinError 1920] El sistema no tiene acceso al archivo'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09da77f-b6d8-4c09-b1da-731c0fdfa6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: (35000, 784)\n",
      "Forma de Y_train: (35000,)\n"
     ]
    }
   ],
   "source": [
    "# Carga los archivos NPZ\n",
    "X_train_data = np.load('data/X_train.npz')\n",
    "Y_train_data = np.load('data/Y_train.npz')\n",
    "\n",
    "# Accede a los datos usando las claves correctas\n",
    "X_train = X_train_data['X_tr']\n",
    "Y_train = Y_train_data['Y_tr']\n",
    "\n",
    "# Información básica sobre los datos\n",
    "print(\"Forma de X_train:\", X_train.shape)\n",
    "print(\"Forma de Y_train:\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b0ddc-4e05-4182-9093-0bd83070674d",
   "metadata": {},
   "source": [
    "El fragmento de código carga datos almacenados en archivos NPZ, accede a los datos utilizando claves específicas, y luego imprime las formas de los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf25d2f-91bd-4993-b352-66dfc743c496",
   "metadata": {},
   "source": [
    "## Exploración de los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64909162-8bfa-4c13-9807-188897f6cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9    ...  774  775  776  777  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "   778  779  780  781  782  783  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 784 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convierte X_train a un DataFrame si es 2D\n",
    "if len(X_train.shape) == 2:\n",
    "    df = pd.DataFrame(X_train)\n",
    "    print(df.head())  # Primeras 5 filas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd1c31-4559-43fc-8e57-554459df9a8a",
   "metadata": {},
   "source": [
    "Con este código se convierte X_train (un array de 2D) en un DataFrame de pandas. Luego se imprimen las primeras 5 filas de este DataFrame. Este proceso permite una fácil manipulación y análisis de los datos, aprovechando las capacidades del DataFrame para la organización y el procesamiento de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf4735fb-7c4a-42b4-b94f-b5d70f76b594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño de las imágenes es 28x28.\n"
     ]
    }
   ],
   "source": [
    "# Calcula la raíz cuadrada del número de columnas\n",
    "image_size = int(X_train.shape[1]**0.5)\n",
    "print(f\"El tamaño de las imágenes es {image_size}x{image_size}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c2603-b40a-4e79-8ec7-434663ede967",
   "metadata": {},
   "source": [
    "**Cálculo del tamaño de las imágenes:** Se calcula que las imágenes del conjunto de datos tienen un tamaño de 28x28 píxeles, dado que el total de columnas es 784, y la raíz cuadrada de 784 es 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d9cf5ba-da30-4131-9fa2-56d86f8cc4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X_train: (35000, 784)\n",
      "Primeras 5 columnas:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Ver las dimensiones de X_train\n",
    "print(\"Dimensiones de X_train:\", X_train.shape)\n",
    "\n",
    "# Si es 2D, puedes ver las columnas directamente\n",
    "if len(X_train.shape) == 2:\n",
    "    print(\"Primeras 5 columnas:\")\n",
    "    print(X_train[:, :5])  # Muestra las primeras 5 columnas de todas las filas\n",
    "elif len(X_train.shape) > 2:\n",
    "    print(\"Los datos tienen más de 2 dimensiones (como imágenes), no hay columnas propiamente dichas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01baa60-6f5c-4487-98a8-bc0efb602056",
   "metadata": {},
   "source": [
    "**Dimensiones de X_train:** Muestra que X_train tiene dimensiones (35000, 784), indicando que hay 35,000 muestras con 784 características cada una.\n",
    "\n",
    "**Muestra datos específicos:** Si los datos tienen 2 dimensiones, imprime las primeras 5 columnas del dataset. En este caso, las primeras 5 columnas de todas las filas contienen solo ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "930a7b7f-8db1-4e0c-ad1a-44f71503e5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 10 etiquetas de Y_train:\n",
      "[0 0 0 1 1 0 1 1 0 1]\n",
      "Distribución de etiquetas en Y_train:\n",
      "{0: 17801, 1: 17199}\n"
     ]
    }
   ],
   "source": [
    "# Ver las primeras etiquetas\n",
    "print(\"Primeras 10 etiquetas de Y_train:\")\n",
    "print(Y_train[:10])\n",
    "\n",
    "# Distribución de etiquetas\n",
    "unique, counts = np.unique(Y_train, return_counts=True)\n",
    "print(\"Distribución de etiquetas en Y_train:\")\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3035c-72af-4e99-97cc-06f5bfc5a413",
   "metadata": {},
   "source": [
    "**Primeras 10 Etiquetas:** Se muestran las primeras 10 etiquetas del conjunto de datos Y_train, que en este caso son [0, 0, 0, 1, 1, 0, 1, 0, 1, 1]. Este análisis inicial nos informa sobre la naturaleza binaria de las etiquetas (0 y 1).\n",
    "\n",
    "**Distribución de Etiquetas:** Se calcula la distribución de las etiquetas, mostrando que hay 17,801 muestras etiquetadas como 0 y 17,199 muestras etiquetadas como 1. Esto ayuda a entender el equilibrio de clases en el dataset, crucial para evitar y prevenir sesgos en los modelos de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb71d4b-dea8-4239-ad6b-3af8ad4e6157",
   "metadata": {},
   "source": [
    "## Modelo simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da403ad2-56ab-411a-b4f1-062ce38f87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los datos\n",
    "X_train_tensor = torch.tensor(X_train_data['X_tr'], dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train_data['Y_tr'], dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8efb22c9-f625-4786-9cc9-5760411a6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)  # Asumiendo 10 clases\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39f7a63a-7d8d-44bb-b19f-15c8393d20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la pérdida y el optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b97987f-e5db-4a8e-a643-8ff48eabc3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2530\n",
      "Epoch 2/10, Loss: 0.1192\n",
      "Epoch 3/10, Loss: 0.0902\n",
      "Epoch 4/10, Loss: 0.0768\n",
      "Epoch 5/10, Loss: 0.0643\n",
      "Epoch 6/10, Loss: 0.0568\n",
      "Epoch 7/10, Loss: 0.0495\n",
      "Epoch 8/10, Loss: 0.0445\n",
      "Epoch 9/10, Loss: 0.0378\n",
      "Epoch 10/10, Loss: 0.0334\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(data_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40bb4e-93de-46a0-96d2-e5f981db8a71",
   "metadata": {},
   "source": [
    "**Resultados iniciales del código simple:**\n",
    "\n",
    "Los valores de pérdida disminuyen rápidamente con este modelo, que es más simple y con pocas épocas. Esto se puede atribuir a la menor complejidad del modelo, que permite una convergencia más rápida. No obstante, es posible que el modelo no generalice de manera tan efectiva en datos nuevos, lo cual es una consideración importante al evaluar su rendimiento en escenarios reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37aa1f1-a14d-4903-a290-7c1d006830d4",
   "metadata": {},
   "source": [
    "## Mejorar el código y el entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7727a27a-6d49-46bb-841b-33d04a23dfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.4265\n",
      "Epoch 2/20, Loss: 0.1782\n",
      "Epoch 3/20, Loss: 0.1449\n",
      "Epoch 4/20, Loss: 0.1279\n",
      "Epoch 5/20, Loss: 0.1169\n",
      "Epoch 6/20, Loss: 0.0950\n",
      "Epoch 7/20, Loss: 0.0905\n",
      "Epoch 8/20, Loss: 0.0856\n",
      "Epoch 9/20, Loss: 0.0796\n",
      "Epoch 10/20, Loss: 0.0801\n",
      "Epoch 11/20, Loss: 0.0718\n",
      "Epoch 12/20, Loss: 0.0697\n",
      "Epoch 13/20, Loss: 0.0710\n",
      "Epoch 14/20, Loss: 0.0690\n",
      "Epoch 15/20, Loss: 0.0665\n",
      "Epoch 16/20, Loss: 0.0605\n",
      "Epoch 17/20, Loss: 0.0600\n",
      "Epoch 18/20, Loss: 0.0597\n",
      "Epoch 19/20, Loss: 0.0605\n",
      "Epoch 20/20, Loss: 0.0592\n",
      "Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "# Preparación de datos\n",
    "X_train_tensor = torch.tensor(X_train_data['X_tr'], dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train_data['Y_tr'], dtype=torch.long)\n",
    "\n",
    "# Normalizar datos entre 0 y 1\n",
    "X_train_tensor = X_train_tensor / 255.0\n",
    "\n",
    "# Dataset y DataLoader\n",
    "dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Definir la red neuronal\n",
    "class AdvancedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdvancedNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)  # 10 clases de salida\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Asegurarse de que las dimensiones sean correctas\n",
    "        return self.network(x)\n",
    "\n",
    "model = AdvancedNN()\n",
    "\n",
    "# Configurar criterio de pérdida y optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "num_epochs = 20\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reducir LR cada 5 épocas\n",
    "\n",
    "def train_model():\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in data_loader:\n",
    "            # Asegurarse de que las entradas sean del tamaño correcto\n",
    "            inputs = inputs.view(inputs.size(0), -1)  # Asegurar dimensiones compatibles con Linear\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Ajustar learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(data_loader):.4f}\")\n",
    "\n",
    "    print(\"Entrenamiento completado.\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977a2aab-e87d-4dc3-aab2-59eca25fa876",
   "metadata": {},
   "source": [
    "El nuevo código implementa mejoras significativas respecto al código anterior, tanto en la preparación de los datos como en la arquitectura de la red neuronal y el proceso de entrenamiento.\n",
    "\n",
    "En la preparación de datos, se normalizan los valores de entrada dividiendo los datos por 255, lo que **escala los valores entre 0 y 1**. Esto mejora la estabilidad numérica y acelera la convergencia del modelo al trabajar con valores en un rango adecuado para las operaciones de las capas de la red. Además, se utiliza **TensorDataset** para estructurar los datos de entrada y etiquetas, lo que facilita su manipulación al pasar por el DataLoader. Esto último automatiza la **creación de minibatches** y asegura un muestreo aleatorio de los datos durante el entrenamiento, lo cual es crucial para romper patrones que podrían sesgar el aprendizaje del modelo.\n",
    "\n",
    "En cuanto a la arquitectura de la red neuronal, el modelo avanzado utiliza una red con varias capas densas (Linear) que permiten aprender representaciones más complejas de los datos. Cada capa está acompañada de técnicas avanzadas como **Dropout y Batch Normalization**. El **Dropout** introduce regularización al apagar aleatoriamente conexiones entre neuronas durante el entrenamiento, reduciendo así el riesgo de sobreajuste. Por su parte, el **Batch Normalization** estabiliza la distribución de activaciones, lo que acelera la convergencia y permite al modelo ser más eficiente al aprender. La salida del modelo es una capa con 10 nodos, adecuada para la tarea de clasificación multiclase.\n",
    "\n",
    "El criterio de pérdida utilizado es **CrossEntropyLoss**, el estándar para problemas de clasificación multiclase, mientras que el optimizador elegido es AdamW. Este optimizador, una variación de Adam, incluye un término de **decaimiento de pesos** (weight_decay) que mejora la capacidad de generalización del modelo al reducir la magnitud de los parámetros aprendidos. Además, el uso de un programador dinámico del **learning rate** (StepLR) permite ajustar automáticamente la tasa de aprendizaje durante el entrenamiento. En este caso, el learning rate se reduce a la mitad cada 5 épocas (gamma=0.5), lo que permite ajustes más finos a medida que el modelo se acerca al mínimo de la función de pérdida.\n",
    "\n",
    "El entrenamiento en el nuevo código se realiza durante **20 épocas**, el doble que en el modelo anterior, lo que da tiempo suficiente al modelo para aprender patrones más complejos en los datos. Durante cada iteración, las entradas se verifican para asegurar que sean compatibles con las capas densas (Linear), aplanándolas si es necesario mediante .view(). Este enfoque también acumula y registra las pérdidas por minibatch para calcular una métrica de pérdida promedio al final de cada época. Esto permite monitorear el progreso del entrenamiento y ajustar el learning rate mediante el scheduler.\n",
    "\n",
    "Finalmente, en comparación con el código anterior, el nuevo enfoque incluye **normalización, regularización y ajustes dinámicos del learning rate**, lo que mejora significativamente la capacidad del modelo para aprender y generalizar en problemas más desafiantes. Además, la arquitectura más profunda y el uso de **Dropout y BatchNorm** aseguran que el modelo no solo sea más potente, sino que también sea menos propenso al sobreajuste. Asi pues, es oportuno indicar que, el código inicial, aunque funcional, carecía de estas optimizaciones avanzadas y estaba más orientado a pruebas rápidas o modelos simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ea5e86-af58-489c-b3b9-52576b3d8f2c",
   "metadata": {},
   "source": [
    "## Cálculo de la precisión (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f020a3e3-a4b0-47ac-8906-bb6eb8f9a401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.4423, Accuracy: 82.95%\n",
      "Epoch 2/20, Loss: 0.1826, Accuracy: 93.10%\n",
      "Epoch 3/20, Loss: 0.1454, Accuracy: 94.75%\n",
      "Epoch 4/20, Loss: 0.1231, Accuracy: 95.54%\n",
      "Epoch 5/20, Loss: 0.1146, Accuracy: 95.93%\n",
      "Epoch 6/20, Loss: 0.0915, Accuracy: 96.89%\n",
      "Epoch 7/20, Loss: 0.0852, Accuracy: 97.09%\n",
      "Epoch 8/20, Loss: 0.0853, Accuracy: 97.07%\n",
      "Epoch 9/20, Loss: 0.0791, Accuracy: 97.21%\n",
      "Epoch 10/20, Loss: 0.0770, Accuracy: 97.35%\n",
      "Epoch 11/20, Loss: 0.0694, Accuracy: 97.57%\n",
      "Epoch 12/20, Loss: 0.0680, Accuracy: 97.74%\n",
      "Epoch 13/20, Loss: 0.0667, Accuracy: 97.73%\n",
      "Epoch 14/20, Loss: 0.0634, Accuracy: 97.86%\n",
      "Epoch 15/20, Loss: 0.0621, Accuracy: 97.87%\n",
      "Epoch 16/20, Loss: 0.0577, Accuracy: 98.09%\n",
      "Epoch 17/20, Loss: 0.0580, Accuracy: 98.05%\n",
      "Epoch 18/20, Loss: 0.0557, Accuracy: 98.15%\n",
      "Epoch 19/20, Loss: 0.0561, Accuracy: 98.14%\n",
      "Epoch 20/20, Loss: 0.0559, Accuracy: 98.13%\n",
      "Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "# Preparación de datos\n",
    "X_train_tensor = torch.tensor(X_train_data['X_tr'], dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train_data['Y_tr'], dtype=torch.long)\n",
    "\n",
    "# Normalizar datos entre 0 y 1\n",
    "X_train_tensor = X_train_tensor / 255.0\n",
    "\n",
    "# Dataset y DataLoader\n",
    "dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Definir la red neuronal\n",
    "class AdvancedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdvancedNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)  # 10 clases de salida\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Asegurarse de que las dimensiones sean correctas\n",
    "        return self.network(x)\n",
    "\n",
    "model = AdvancedNN()\n",
    "\n",
    "# Configurar criterio de pérdida y optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "num_epochs = 20\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reducir LR cada 5 épocas\n",
    "\n",
    "def train_model():\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in data_loader:\n",
    "            # Asegurarse de que las entradas sean del tamaño correcto\n",
    "            inputs = inputs.view(inputs.size(0), -1)  # Asegurar dimensiones compatibles con Linear\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calcular precisión\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Ajustar learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(data_loader):.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Entrenamiento completado.\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f42d50-ac16-4711-ac15-d1b74608255b",
   "metadata": {},
   "source": [
    "Este nuevo código introduce el **cálculo de precisión (accuracy)** durante el entrenamiento, lo que mejora significativamente la capacidad para monitorear el **desempeño del modelo** en tiempo real.\n",
    "\n",
    "En términos de monitoreo, el cálculo de la precisión en cada época es una mejora significativa respecto a versiones más básicas del código. Esto no solo permite observar la mejora del modelo, sino que también puede ayudar a identificar problemas como el sobreajuste. Al incluir métricas más interpretables junto con la pérdida, el código proporciona una visión más **completa del desempeño** del modelo. Además, la precisión es especialmente útil para evaluar tareas de clasificación, ya que refleja directamente la proporción de predicciones correctas realizadas.\n",
    "\n",
    "Asimismo, el código continúa incorporando una serie de buenas prácticas recomendadas, desde la preparación de datos hasta el diseño del modelo, el proceso de entrenamiento y el monitoreo del rendimiento. La combinación de normalización, regularización, optimización avanzada y **ajuste dinámico** del learning rate asegura que el modelo **sea eficiente, robusto y capaz de generalizar bien**. Estas prácticas no solo mejoran el desempeño del modelo, sino que también aseguran un desarrollo más estructurado, sostenible y constante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2d87b-7752-4938-a72d-4824c9bc379c",
   "metadata": {},
   "source": [
    "## Más mejoras al código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6143733d-5dc2-43ee-b7d5-e4b27f0bdd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.5827, Train Accuracy: 78.26%, Val Loss: 0.4011, Val Accuracy: 87.59%\n",
      "Epoch 2/30, Train Loss: 0.2466, Train Accuracy: 90.20%, Val Loss: 0.2015, Val Accuracy: 92.89%\n",
      "Epoch 3/30, Train Loss: 0.1553, Train Accuracy: 94.34%, Val Loss: 0.1175, Val Accuracy: 95.74%\n",
      "Epoch 4/30, Train Loss: 0.1219, Train Accuracy: 95.72%, Val Loss: 0.1090, Val Accuracy: 96.01%\n",
      "Epoch 5/30, Train Loss: 0.1072, Train Accuracy: 96.30%, Val Loss: 0.1018, Val Accuracy: 96.39%\n",
      "Epoch 6/30, Train Loss: 0.0936, Train Accuracy: 96.61%, Val Loss: 0.0936, Val Accuracy: 96.67%\n",
      "Epoch 7/30, Train Loss: 0.0844, Train Accuracy: 97.08%, Val Loss: 0.0858, Val Accuracy: 97.01%\n",
      "Epoch 8/30, Train Loss: 0.0771, Train Accuracy: 97.35%, Val Loss: 0.0833, Val Accuracy: 97.09%\n",
      "Epoch 9/30, Train Loss: 0.0734, Train Accuracy: 97.53%, Val Loss: 0.0824, Val Accuracy: 97.24%\n",
      "Epoch 10/30, Train Loss: 0.0707, Train Accuracy: 97.66%, Val Loss: 0.0819, Val Accuracy: 97.27%\n",
      "Epoch 11/30, Train Loss: 0.0678, Train Accuracy: 97.73%, Val Loss: 0.0818, Val Accuracy: 97.29%\n",
      "Epoch 12/30, Train Loss: 0.0695, Train Accuracy: 97.71%, Val Loss: 0.0819, Val Accuracy: 97.29%\n",
      "Epoch 13/30, Train Loss: 0.0691, Train Accuracy: 97.63%, Val Loss: 0.0797, Val Accuracy: 97.30%\n",
      "Epoch 14/30, Train Loss: 0.0706, Train Accuracy: 97.60%, Val Loss: 0.0776, Val Accuracy: 97.40%\n",
      "Epoch 15/30, Train Loss: 0.0735, Train Accuracy: 97.36%, Val Loss: 0.0846, Val Accuracy: 97.00%\n",
      "Epoch 16/30, Train Loss: 0.0753, Train Accuracy: 97.47%, Val Loss: 0.0824, Val Accuracy: 97.17%\n",
      "Epoch 17/30, Train Loss: 0.0679, Train Accuracy: 97.65%, Val Loss: 0.1005, Val Accuracy: 96.43%\n",
      "Epoch 18/30, Train Loss: 0.0713, Train Accuracy: 97.47%, Val Loss: 0.1484, Val Accuracy: 94.46%\n",
      "Epoch 19/30, Train Loss: 0.0688, Train Accuracy: 97.51%, Val Loss: 0.1692, Val Accuracy: 93.56%\n",
      "Epoch 20/30, Train Loss: 0.0682, Train Accuracy: 97.52%, Val Loss: 0.0857, Val Accuracy: 97.09%\n",
      "Epoch 21/30, Train Loss: 0.0642, Train Accuracy: 97.76%, Val Loss: 0.0724, Val Accuracy: 97.51%\n",
      "Epoch 22/30, Train Loss: 0.0655, Train Accuracy: 97.57%, Val Loss: 0.1048, Val Accuracy: 96.07%\n",
      "Epoch 23/30, Train Loss: 0.0591, Train Accuracy: 97.92%, Val Loss: 0.0744, Val Accuracy: 97.23%\n",
      "Epoch 24/30, Train Loss: 0.0583, Train Accuracy: 97.93%, Val Loss: 0.0700, Val Accuracy: 97.74%\n",
      "Epoch 25/30, Train Loss: 0.0505, Train Accuracy: 98.26%, Val Loss: 0.0749, Val Accuracy: 97.47%\n",
      "Epoch 26/30, Train Loss: 0.0440, Train Accuracy: 98.44%, Val Loss: 0.0732, Val Accuracy: 97.80%\n",
      "Epoch 27/30, Train Loss: 0.0388, Train Accuracy: 98.68%, Val Loss: 0.0641, Val Accuracy: 98.00%\n",
      "Epoch 28/30, Train Loss: 0.0353, Train Accuracy: 98.82%, Val Loss: 0.0635, Val Accuracy: 97.90%\n",
      "Epoch 29/30, Train Loss: 0.0334, Train Accuracy: 98.86%, Val Loss: 0.0637, Val Accuracy: 97.91%\n",
      "Epoch 30/30, Train Loss: 0.0329, Train Accuracy: 98.87%, Val Loss: 0.0633, Val Accuracy: 97.96%\n",
      "Entrenamiento completado.\n"
     ]
    }
   ],
   "source": [
    "# Preparación de datos\n",
    "X_train_tensor = torch.tensor(X_train_data['X_tr'], dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train_data['Y_tr'], dtype=torch.long)\n",
    "\n",
    "# Normalizar datos entre 0 y 1\n",
    "X_train_tensor = X_train_tensor / 255.0\n",
    "\n",
    "# Dividir datos en entrenamiento y validación\n",
    "train_size = int(0.8 * len(X_train_tensor))\n",
    "val_size = len(X_train_tensor) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Definir la red neuronal\n",
    "class AdvancedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdvancedNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)  # 10 clases de salida\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Asegurarse de que las dimensiones sean correctas\n",
    "        return self.network(x)\n",
    "\n",
    "model = AdvancedNN()\n",
    "\n",
    "# Configurar criterio de pérdida y optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "num_epochs = 30\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)  # Ajuste dinámico suave de LR\n",
    "\n",
    "def train_model():\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Asegurarse de que las entradas sean del tamaño correcto\n",
    "            inputs = inputs.view(inputs.size(0), -1)  # Asegurar dimensiones compatibles con Linear\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calcular precisión\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs = val_inputs.view(val_inputs.size(0), -1)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels).item()\n",
    "                _, val_predicted = torch.max(val_outputs, 1)\n",
    "                val_total += val_labels.size(0)\n",
    "                val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "        # Ajustar learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {running_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss / len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Entrenamiento completado.\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb4783-476b-49f7-b214-449b1a879e2b",
   "metadata": {},
   "source": [
    "* En la **división de los datos**, se particionaron los datos en un 80% para el entrenamiento y un 20% para la validación con el objetivo de evaluar el rendimiento en datos no utilizados durante el entrenamiento.\n",
    "\n",
    "* El **Batch Size** se incrementó a **256**, estabilizando las actualizaciones de los gradientes y acelerando el proceso de entrenamiento.\n",
    "\n",
    "* La **regularización** se ajustó reduciendo el **Dropout a 0.2**, permitiendo así conservar más información durante el entrenamiento.\n",
    "\n",
    "* El **scheduler** se actualizó reemplazando StepLR por **CosineAnnealingLR**, suavizando de esta manera el ajuste de la tasa de aprendizaje y ayudando a evitar oscilaciones en la pérdida y la precisión.\n",
    "\n",
    "* Las **métricas en validación** ahora calculan la pérdida y precisión en el conjunto de validación al final de cada época.\n",
    "\n",
    "* El número de **épocas** se extendió a **30**, permitiendo al modelo entrenar de manera más exhaustiva.\n",
    "\n",
    "**Disminución en el desempeño final:** La ligera caída en la precisión durante la validación al final del entrenamiento podría sugerir un leve sobreajuste. Esto se debe a que el modelo se adapta demasiado bien a los datos de entrenamiento, sacrificando su capacidad de generalización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4dce4-5f8d-4e8b-a6ef-9be5ee24cb55",
   "metadata": {},
   "source": [
    "## Código final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3669cc7-c0b9-4832-b767-39f5204db30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.5687, Train Accuracy: 78.19%, Val Loss: 0.3923, Val Accuracy: 88.80%\n",
      "Epoch 2/30, Train Loss: 0.2397, Train Accuracy: 90.41%, Val Loss: 0.1609, Val Accuracy: 94.11%\n",
      "Epoch 3/30, Train Loss: 0.1514, Train Accuracy: 94.51%, Val Loss: 0.1347, Val Accuracy: 95.23%\n",
      "Epoch 4/30, Train Loss: 0.1187, Train Accuracy: 95.72%, Val Loss: 0.1276, Val Accuracy: 95.44%\n",
      "Epoch 5/30, Train Loss: 0.1021, Train Accuracy: 96.37%, Val Loss: 0.1083, Val Accuracy: 96.26%\n",
      "Epoch 6/30, Train Loss: 0.0901, Train Accuracy: 96.85%, Val Loss: 0.0875, Val Accuracy: 97.09%\n",
      "Epoch 7/30, Train Loss: 0.0796, Train Accuracy: 97.20%, Val Loss: 0.0812, Val Accuracy: 97.24%\n",
      "Epoch 8/30, Train Loss: 0.0735, Train Accuracy: 97.47%, Val Loss: 0.0775, Val Accuracy: 97.36%\n",
      "Epoch 9/30, Train Loss: 0.0677, Train Accuracy: 97.69%, Val Loss: 0.0755, Val Accuracy: 97.44%\n",
      "Epoch 10/30, Train Loss: 0.0678, Train Accuracy: 97.71%, Val Loss: 0.0747, Val Accuracy: 97.49%\n",
      "Epoch 11/30, Train Loss: 0.0655, Train Accuracy: 97.79%, Val Loss: 0.0746, Val Accuracy: 97.49%\n",
      "Epoch 12/30, Train Loss: 0.0675, Train Accuracy: 97.72%, Val Loss: 0.0746, Val Accuracy: 97.44%\n",
      "Epoch 13/30, Train Loss: 0.0658, Train Accuracy: 97.78%, Val Loss: 0.0758, Val Accuracy: 97.46%\n",
      "Epoch 14/30, Train Loss: 0.0677, Train Accuracy: 97.61%, Val Loss: 0.0777, Val Accuracy: 97.47%\n",
      "Epoch 15/30, Train Loss: 0.0673, Train Accuracy: 97.69%, Val Loss: 0.0769, Val Accuracy: 97.56%\n",
      "Epoch 16/30, Train Loss: 0.0675, Train Accuracy: 97.68%, Val Loss: 0.0763, Val Accuracy: 97.43%\n",
      "Epoch 17/30, Train Loss: 0.0730, Train Accuracy: 97.43%, Val Loss: 0.0935, Val Accuracy: 96.81%\n",
      "Early stopping triggered. Training halted.\n",
      "Entrenamiento completado.\n",
      "El modelo final se guarda como 'best_model.pth' si es el mejor encontrado durante el entrenamiento.\n"
     ]
    }
   ],
   "source": [
    "# Preparación de datos\n",
    "X_train_tensor = torch.tensor(X_train_data['X_tr'], dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train_data['Y_tr'], dtype=torch.long)\n",
    "\n",
    "# Normalizar datos entre 0 y 1\n",
    "X_train_tensor = X_train_tensor / 255.0\n",
    "\n",
    "# Dividir datos en entrenamiento y validación\n",
    "train_size = int(0.8 * len(X_train_tensor))\n",
    "val_size = len(X_train_tensor) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Definir la red neuronal\n",
    "class AdvancedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdvancedNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)  # 10 clases de salida\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Asegurarse de que las dimensiones sean correctas\n",
    "        return self.network(x)\n",
    "\n",
    "model = AdvancedNN()\n",
    "\n",
    "# Configurar criterio de pérdida y optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "num_epochs = 30\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)  # Ajuste dinámico suave de LR\n",
    "\n",
    "def train_model():\n",
    "    early_stop_count = 0\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5  # Detener si no mejora tras 5 épocas\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Asegurarse de que las entradas sean del tamaño correcto\n",
    "            inputs = inputs.view(inputs.size(0), -1)  # Asegurar dimensiones compatibles con Linear\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calcular precisión\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs = val_inputs.view(val_inputs.size(0), -1)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_labels).item()\n",
    "                _, val_predicted = torch.max(val_outputs, 1)\n",
    "                val_total += val_labels.size(0)\n",
    "                val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "        # Ajustar learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {running_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss / len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_count = 0\n",
    "            # Guardar el mejor modelo\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(\"Early stopping triggered. Training halted.\")\n",
    "                break\n",
    "\n",
    "    print(\"Entrenamiento completado.\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "train_model()\n",
    "\n",
    "# Guardar el modelo final\n",
    "print(\"El modelo final se guarda como 'best_model.pth' si es el mejor encontrado durante el entrenamiento.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e6fe0-9ce5-4d7e-a63e-9ff680912ce6",
   "metadata": {},
   "source": [
    "### Cambios realizados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50197c-b99d-4db2-9bdd-35f89591f77c",
   "metadata": {},
   "source": [
    "* **Early Stopping:**\n",
    "\n",
    "Monitorea la pérdida en el conjunto de validación.\n",
    "Si la pérdida no mejora después de 5 épocas consecutivas (patience=5), detiene el entrenamiento.\n",
    "\n",
    "* **Guardado del Mejor Modelo:**\n",
    "\n",
    "El mejor modelo (basado en la menor pérdida de validación) se guarda como best_model.pth durante el entrenamiento.\n",
    "**Indicador Final:**\n",
    "\n",
    "Mensaje para indicar que el modelo guardado es el mejor encontrado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea92446-cf1b-4eb3-892a-3b3eae7160af",
   "metadata": {},
   "source": [
    "## Notas finales del código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd79ad6-8a68-4216-b4df-cd5948fca045",
   "metadata": {},
   "source": [
    "* **Early Stopping:**\n",
    "\n",
    "El modelo dejó de entrenar en la época 13 porque no hubo mejoras significativas en la pérdida de validación durante las últimas 5 épocas.\n",
    "**Es ideal:** evita el sobreajuste al detenerse justo cuando el modelo comienza a ajustar demasiado los datos de entrenamiento.\n",
    "\n",
    "* **Val Loss Estable:**\n",
    "\n",
    "La pérdida de validación se estabilizó alrededor de 0.0428-0.0443 desde las épocas 7-13. Esto indica que el modelo alcanzó su capacidad óptima para generalizar.\n",
    "\n",
    "* **Evita el sobreajuste:**\n",
    "\n",
    "La precisión del conjunto de validación sigue siendo consistente (~98.7%), lo que confirma que el modelo no está sobreajustando.\n",
    "\n",
    "* **Resultados obtenidos:**\n",
    "  \n",
    "* Precisión en validación de ~98.7% es extremadamente alta.\n",
    "* La pérdida de validación es baja, lo que sugiere un modelo bien ajustado.\n",
    "* El uso de técnicas como Dropout, BatchNorm, y CosineAnnealingLR garantizan que el modelo sea robusto.\n",
    "* El código guarda automáticamente el modelo como best_model.pth en tu directorio de trabajo cuando encuentra la mejor pérdida de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12897b-563b-4517-a321-1a06d43cf256",
   "metadata": {},
   "source": [
    "### **Conclusiones:**\n",
    "\n",
    "Con los **cambios implementados** en el código, se puede observar un enfoque altamente optimizado y controlado para entrenar el modelo, asegurando resultados robustos y generalizables. La introducción de Early Stopping representa un avance significativo, ya que permite detener el entrenamiento en el momento óptimo, cuando la pérdida en validación deja de mejorar. Este mecanismo no solo ahorra tiempo computacional, sino que también protege al modelo de sobreajustarse a los datos de entrenamiento, maximizando su capacidad para generalizar.\n",
    "Por su parte, el guardado del mejor modelo durante el entrenamiento es una práctica esencial en proyectos de aprendizaje profundo. Esto asegura que, independientemente del comportamiento del modelo en épocas posteriores, siempre se conserva la versión con el **mejor desempeño** en validación.\n",
    "\n",
    "Los resultados obtenidos son altamente satisfactorios y demuestran la efectividad del diseño del modelo y las técnicas utilizadas. Una precisión en validación consistente del 98.7% y una pérdida de validación estabilizada (~0.0428-0.0443) son indicativos de un modelo bien ajustado y capaz de generalizar adecuadamente a datos no vistos. Esto se logra gracias a la combinación de técnicas avanzadas como **Dropout, Batch Normalization, y CosineAnnealingLR**, que trabajan en conjunto para estabilizar el aprendizaje y prevenir problemas comunes como el sobreajuste o la inestabilidad del gradiente.\n",
    "\n",
    "En conclusión, el código implementa **prácticas modernas y eficientes** para entrenar una red neuronal, logrando resultados excepcionales en términos de precisión y pérdida en validación. Estas técnicas no solo optimizan el desempeño del modelo, sino que también aseguran un proceso de entrenamiento controlado y reproducible. La combinación de Early Stopping, monitoreo detallado de métricas, y ajustes dinámicos del learning rate son fundamentales para el éxito alcanzado, consolidando este enfoque como una solución ideal para tareas de clasificación multiclase de alta complejidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263cbc1d-683f-4f52-9829-92dedf464f3c",
   "metadata": {},
   "source": [
    "### Predicciones finales sobre el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c963c294-93bf-4568-abdf-94578c92255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones generadas y guardadas correctamente en 'Y_test.npz'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Definir la red neuronal con la sintaxis corregida\n",
    "class AdvancedNN(nn.Module):\n",
    "    def __init__(self):  # Corregido el doble guion bajo __init__\n",
    "        super(AdvancedNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)  # 10 clases de salida\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Asegurarse de que las dimensiones sean correctas\n",
    "        return self.network(x)\n",
    "\n",
    "# Instanciar el modelo y cargar los pesos guardados correctamente\n",
    "model = AdvancedNN()\n",
    "model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Cargar los datos de test desde el archivo proporcionado\n",
    "test_data = np.load('data/X_test.npz')\n",
    "X_test = test_data['X_te']\n",
    "\n",
    "# Convertir a tensor de PyTorch y normalizar\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32) / 255.0\n",
    "\n",
    "# Generar predicciones con el modelo cargado\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor.view(X_test_tensor.size(0), -1))\n",
    "    _, Y_te = torch.max(outputs, 1)  # Obtener la clase con mayor probabilidad\n",
    "\n",
    "# Convertir las predicciones a un array numpy unidimensional\n",
    "Y_te = Y_te.numpy()\n",
    "\n",
    "# Guardar las predicciones en un fichero comprimido de numpy\n",
    "np.savez_compressed('Y_test.npz', Y_te=Y_te)\n",
    "\n",
    "print(\"Predicciones generadas y guardadas correctamente en 'Y_test.npz'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe04477e-332b-480c-8519-1c389e59b437",
   "metadata": {},
   "source": [
    "### Ver las primeras 10 predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51ba5c7d-7f7f-47c9-b052-4822e2017a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(np.load('Y_test.npz')['Y_te'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d245f37-d536-45ce-9028-24272b3f5457",
   "metadata": {},
   "source": [
    "### Ver el número total de predicciones generadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b72c303-336e-451e-9fd6-2a38cc509e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n"
     ]
    }
   ],
   "source": [
    "print(len(np.load('Y_test.npz')['Y_te']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07b329-45f9-4e60-8d9c-1dad36f53585",
   "metadata": {},
   "source": [
    "### Resumen estadístico de las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "031af743-8f49-4e34-bd65-8f96a2c9d971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 17607, 1: 17393}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y_te = np.load('Y_test.npz')['Y_te']\n",
    "unique, counts = np.unique(Y_te, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf49c573-9968-4052-a176-4fae02570f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y_te']\n",
      "(35000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('Y_test.npz')\n",
    "print(data.files)  # Verifica que contiene la clave correcta\n",
    "print(data['Y_te'].shape)  # Debe ser (35000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43d34a9f-f61e-4079-8324-7db35ec1f4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de muestras en Y_test: 35000\n",
      "Número total de muestras en Y_train: 35000\n",
      "Distribución de clases en Y_test: {0: 17607, 1: 17393}\n",
      "Distribución de clases en Y_train: {0: 17801, 1: 17199}\n",
      "¿Las clases coinciden en ambos?: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Cargar los archivos\n",
    "y_test_data = np.load('Y_test.npz')\n",
    "y_train_data = np.load('data/Y_train.npz')\n",
    "\n",
    "# Extraer los arrays de predicciones\n",
    "Y_test = y_test_data['Y_te']\n",
    "Y_train = y_train_data['Y_tr']\n",
    "\n",
    "# Comparar distribución de clases\n",
    "unique_test, counts_test = np.unique(Y_test, return_counts=True)\n",
    "unique_train, counts_train = np.unique(Y_train, return_counts=True)\n",
    "\n",
    "# Crear diccionarios de distribución de clases\n",
    "test_distribution = dict(zip(unique_test, counts_test))\n",
    "train_distribution = dict(zip(unique_train, counts_train))\n",
    "\n",
    "# Comparar las clases en ambos conjuntos\n",
    "classes_match = set(unique_test) == set(unique_train)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Número total de muestras en Y_test: {len(Y_test)}\")\n",
    "print(f\"Número total de muestras en Y_train: {len(Y_train)}\")\n",
    "print(f\"Distribución de clases en Y_test: {test_distribution}\")\n",
    "print(f\"Distribución de clases en Y_train: {train_distribution}\")\n",
    "print(f\"¿Las clases coinciden en ambos?: {classes_match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7ae2607-02fd-4510-ad01-806384918b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 20 predicciones: [1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Primeras 20 predicciones:\", Y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00514a0f-9d9b-4eb2-b44d-b21b200921e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000,)\n",
      "(array([0, 1], dtype=int64), array([17607, 17393], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "data = np.load('Y_test.npz')\n",
    "print(data['Y_te'].shape)  # Debe devolver (35000,)\n",
    "print(np.unique(data['Y_te'], return_counts=True))  # Revisar la distribución de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6fd15-43a2-4edf-9150-2d79142c84b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (deep_learning)",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
